{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11854237,"sourceType":"datasetVersion","datasetId":7448713}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Trainingsetè³‡æ–™å‰è™•ç†","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport shap\n\n# === 1. å…¨å±€ SEEDï¼ˆç¢ºä¿ deterministicï¼‰===\nGLOBAL_SEED = 42\nrandom.seed(GLOBAL_SEED)\nnp.random.seed(GLOBAL_SEED)\n\n# === 2. åŸºæœ¬åƒæ•¸èˆ‡è·¯å¾‘ ===\nMODEL_DIR = '/kaggle/working'\nos.makedirs(MODEL_DIR, exist_ok=True)\nmax_rul = 130      # æœ€å¤§RULæˆªæ–·\nwindow_size = 12   # é€€åŒ–èµ·é»åµæ¸¬ window\nthreshold = 0.2    # é€€åŒ–åµæ¸¬ threshold\n\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\n\n# === 3. è³‡æ–™è¼‰å…¥èˆ‡åˆæ­¥RULæ¨™è¨» ===\ndef load_and_label(filepath, max_rul=130):\n    # ç›´æ¥ç”¨ floatï¼Œå¾ŒçºŒéƒ½ä¸ç”¨ç…©æƒ±å‹åˆ¥\n    df = pd.read_csv(filepath, sep='\\s+', header=None, dtype=float)\n    df.columns = columns\n    df['RUL_linear'] = df.groupby('unit')['time'].transform('max') - df['time']\n    df['RUL_linear'] = df['RUL_linear'].clip(upper=max_rul)\n    return df\n\ntrain = load_and_label('/kaggle/input/cmapssdata/train_FD001.txt', max_rul=max_rul)\n\n# === 4. åˆ†æ®µç·šæ€§é€€åŒ–RUL (Algorithm1) ===\ndef generate_piecewise_rul_algorithm1(df, feature_cols, window_size=12, threshold=0.2, max_rul=130, verbose=False):\n    result_dfs = []\n    for unit_id, group in df.groupby('unit'):\n        group = group.sort_values('time').reset_index(drop=True)\n        sensor_data = group[feature_cols].values\n        num_cycles = len(group)\n        num_windows = num_cycles // window_size\n\n        if num_windows < 3:\n            irul = num_cycles\n            group['iRUL'] = irul\n            group['RUL_piecewise'] = np.clip(irul - group['time'], 0, max_rul)\n            result_dfs.append(group)\n            continue\n\n        centroids = [np.mean(sensor_data[i * window_size:(i + 1) * window_size], axis=0)\n                     for i in range(num_windows)]\n        base = centroids[0]\n        degradation_found = False\n\n        for i in range(2, num_windows):\n            dist_sq = np.sum((centroids[i] - base) ** 2)\n            if verbose:\n                print(f\"[Unit {unit_id}] Compare w1 to w{i+1}: Dist^2 = {dist_sq:.4f}\")\n            if dist_sq >= threshold:\n                degradation_start = i * window_size\n                irul = num_cycles - degradation_start\n                degradation_found = True\n                break\n\n        if not degradation_found:\n            irul = num_cycles\n\n        rul_piecewise = [\n            irul if t <= (num_cycles - irul) else irul - (t - (num_cycles - irul))\n            for t in range(num_cycles)\n        ]\n        group['iRUL'] = irul\n        group['RUL_piecewise'] = np.clip(rul_piecewise, 0, max_rul)\n        result_dfs.append(group)\n\n    return pd.concat(result_dfs, ignore_index=True)\n\n# === 5. åŸºç¤å‰è™•ç†ï¼ˆç§»é™¤ä½è®Šç•°æ„Ÿæ¸¬å™¨ï¼‰===\nsensor_cols = [f's{i}' for i in range(1, 22)]\nsensor_cols_keep = [col for col in sensor_cols if train[col].std() >= 1e-3]\n\n# === 6. Z-score éæ¿¾é›¢ç¾¤å€¼ ===\ndef z_score_filter(df, cols, threshold=4.0):\n    for col in cols:\n        mean = df[col].mean()\n        std = df[col].std()\n        z = (df[col] - mean) / std\n        df.loc[z.abs() > threshold, col] = mean\n    return df\n\ntrain = z_score_filter(train, sensor_cols_keep)\n\n# === 7. åˆ†æ®µç·šæ€§RULæ¨™è¨» ===\ntrain = generate_piecewise_rul_algorithm1(\n    train, sensor_cols_keep, window_size=window_size, threshold=threshold, max_rul=max_rul, verbose=False\n)\n\n# === 8. ç‰¹å¾µé¸æ“‡ï¼ˆSHAP + çš®çˆ¾æ£®ï¼‰===\nrf = RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1)\nrf.fit(train[sensor_cols_keep], train['RUL_piecewise'])\n\nexplainer = shap.Explainer(rf, train[sensor_cols_keep].iloc[:500])\nshap_values = explainer(train[sensor_cols_keep].iloc[:500])\nmean_shap = np.abs(shap_values.values).mean(axis=0)\nshap_scores = pd.Series(mean_shap, index=sensor_cols_keep).sort_values(ascending=False)\ntop8_shap = shap_scores.head(8).index.tolist()\n\ncorrs = train[sensor_cols_keep + ['RUL_piecewise']].corr()['RUL_piecewise'].abs().sort_values(ascending=False)\ntop8_pearson = corrs.drop('RUL_piecewise').head(8).index.tolist()\n\nselected_sensors = [x for x in top8_shap if x in top8_pearson]\nfor x in top8_pearson:\n    if x not in selected_sensors:\n        selected_sensors.append(x)\n    if len(selected_sensors) == 8:\n        break\n\nprint(\"æœ€çµ‚é¸ç”¨çš„8å€‹æ„Ÿæ¸¬å™¨:\", selected_sensors)\n\n# === 9. æ§‹å»ºæœ€çµ‚ç‰¹å¾µï¼ˆå«OP/å·®åˆ†ï¼‰===\nfeatures = selected_sensors + ['op1', 'op2', 'op3']\n\ndef multi_exponential_smoothing(series, alphas=[0.1, 0.3]):\n    results = []\n    for alpha in alphas:\n        smoothed = [series.iloc[0]]\n        for n in range(1, len(series)):\n            smoothed.append(alpha * series.iloc[n] + (1 - alpha) * smoothed[-1])\n        results.append(pd.Series(smoothed, index=series.index))\n    return sum(results) / len(results)\n\nfor col in features:\n    train[col] = train.groupby('unit')[col].transform(lambda x: multi_exponential_smoothing(x)).astype('float64')\n\nfor col in selected_sensors:\n    train[f'{col}_diff'] = train.groupby('unit')[col].diff().fillna(0)\n\nfinal_features = features + [f'{col}_diff' for col in selected_sensors]\n\n# === 10. æ¨™æº–åŒ– ===\nscaler = StandardScaler()\ntrain[final_features] = scaler.fit_transform(train[final_features])\n\n# === 11. å„²å­˜æ‰€æœ‰é‡è¦è³‡è¨Š ===\njoblib.dump(scaler, f'{MODEL_DIR}/scaler_preprocessed.joblib')\njoblib.dump(final_features, f'{MODEL_DIR}/feature_names.pkl')\ntrain.to_csv(f'{MODEL_DIR}/train_with_piecewise_rul.csv', index=False)\n\nprint('ğŸš© å·²å®Œæˆåˆ†æ®µç·šæ€§RULè³‡æ–™èˆ‡æ‰€æœ‰ç‰¹å¾µè™•ç†ï¼Œæª”æ¡ˆå„²å­˜ï¼š')\nprint(f'- {MODEL_DIR}/train_with_piecewise_rul.csv')\nprint(f'- {MODEL_DIR}/feature_names.pkl')\nprint(f'- {MODEL_DIR}/scaler_preprocessed.joblib')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:53:32.503100Z","iopub.execute_input":"2025-05-31T13:53:32.503390Z","iopub.status.idle":"2025-05-31T13:54:47.880384Z","shell.execute_reply.started":"2025-05-31T13:53:32.503370Z","shell.execute_reply":"2025-05-31T13:54:47.879541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FD001è¨“ç·´","metadata":{}},{"cell_type":"code","source":"!pip install optuna optuna-integration[tfkeras] -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:54:47.881541Z","iopub.execute_input":"2025-05-31T13:54:47.881775Z","iopub.status.idle":"2025-05-31T13:54:53.311851Z","shell.execute_reply.started":"2025-05-31T13:54:47.881758Z","shell.execute_reply":"2025-05-31T13:54:53.311183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # åªé¡¯ç¤ºéŒ¯èª¤\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport optuna\nfrom optuna_integration import TFKerasPruningCallback\n\n# ====== 1. è¨­å®šåƒæ•¸ ======\nDATASET = 'fd001'\nVALIDATION_SPLIT = 0.2\nGLOBAL_SEED = 42\n\nMODEL_DIR = '/kaggle/working'\nDATA_PATH = f'{MODEL_DIR}/train_with_piecewise_rul.csv'\nSCALER_PATH = f'{MODEL_DIR}/scaler_preprocessed.joblib'\nFEATURE_PATH = f'{MODEL_DIR}/feature_names.pkl'\nMODEL_PATH = f'{MODEL_DIR}/best_{DATASET}_lstm_model_mse.keras'\nRESULT_CSV = f'{MODEL_DIR}/{DATASET}_optuna_search_results.csv'\nVAL_UNITS_PATH = f'{MODEL_DIR}/{DATASET}_val_units.npy'\nTRAIN_UNITS_PATH = f'{MODEL_DIR}/{DATASET}_train_units.npy'\n\nnp.random.seed(GLOBAL_SEED)\ntf.random.set_seed(GLOBAL_SEED)\n\n# ====== 2. è®€è³‡æ–™èˆ‡ç‰¹å¾µï¼ˆå®Œå…¨å°æ‡‰å‰è™•ç†è¼¸å‡ºï¼‰======\nfeature_cols = joblib.load(FEATURE_PATH)\nscaler = joblib.load(SCALER_PATH)  # é è™•ç†éšæ®µå·²ç¶“fitå¥½\n\ndf_raw = pd.read_csv(DATA_PATH)\nrul_col = \"RUL_piecewise\"\nmin_irul = df_raw['iRUL'].min()\nnp.save(f'{MODEL_DIR}/{DATASET}_min_irul.npy', min_irul)\nprint(f\"ğŸ“‰ ä½¿ç”¨å‹•æ…‹ CLIP RULï¼ˆæœ€å° iRULï¼‰: {min_irul}\")\n\nall_units = df_raw['unit'].unique()\nrng = np.random.default_rng(GLOBAL_SEED)\n\nif os.path.exists(VAL_UNITS_PATH) and os.path.exists(TRAIN_UNITS_PATH):\n    val_units = np.load(VAL_UNITS_PATH)\n    train_units = np.load(TRAIN_UNITS_PATH)\n    print(\"âœ… å·²è¼‰å…¥ç¾æœ‰çš„ validation/train åˆ†å‰²åå–®\")\nelse:\n    val_units = rng.choice(all_units, int(len(all_units) * VALIDATION_SPLIT), replace=False)\n    train_units = [u for u in all_units if u not in val_units]\n    np.save(VAL_UNITS_PATH, val_units)\n    np.save(TRAIN_UNITS_PATH, train_units)\n    print(\"âœ… é¦–æ¬¡å»ºç«‹ validation/train åˆ†å‰²ä¸¦å„²å­˜\")\n\ntrain_df = df_raw[df_raw['unit'].isin(train_units)]\nval_df = df_raw[df_raw['unit'].isin(val_units)]\n\n# ====== 3. LSTMè¨“ç·´è³‡æ–™è£½ä½œ ======\ndef make_lstm_dataset(df, feature_cols, window_size, stride=1, target_col='RUL_piecewise', clip_value=None):\n    sequences, labels = [], []\n    for _, group in df.groupby('unit'):\n        data = group.sort_values('time')\n        X = data[feature_cols].values\n        y = data[target_col].values\n        if clip_value is not None:\n            y = np.clip(y, 0, clip_value)\n        for i in range(0, len(data) - window_size + 1, stride):\n            sequences.append(X[i:i+window_size])\n            labels.append(y[i+window_size-1])\n    return np.array(sequences), np.array(labels)\n\ndef phm08_score(y_true, y_pred):\n    error = y_pred - y_true\n    score = np.where(error < 0,\n                     np.exp(-error / 13) - 1,\n                     np.exp(error / 10) - 1)\n    return np.sum(score)\n\ndef build_model(input_shape, lstm_units, dropout_rate, learning_rate, optimizer_name, num_layers):\n    model = Sequential()\n    model.add(tf.keras.layers.Input(shape=input_shape))\n    for i in range(num_layers):\n        return_seq = (i < num_layers - 1)\n        model.add(LSTM(lstm_units, return_sequences=return_seq))\n        model.add(Dropout(dropout_rate))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n    optimizers = {\n        'adam': tf.keras.optimizers.Adam(learning_rate),\n        'sgd': tf.keras.optimizers.SGD(learning_rate),\n        'rmsprop': tf.keras.optimizers.RMSprop(learning_rate),\n        'nadam': tf.keras.optimizers.Nadam(learning_rate)\n    }\n    model.compile(optimizer=optimizers[optimizer_name],\n                  loss='mse',\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model\n\n# ====== 4. Optuna æœå°‹æœ€ä½³è¶…åƒæ•¸ï¼ˆå›ºå®šè³‡æ–™åˆ†å‰²ï¼‰======\ndef objective(trial):\n    seed = trial.suggest_categorical('seed', [11,22,33,44])\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    lstm_units = trial.suggest_categorical('lstm_units', [32, 64, 96, 128])\n    dropout_rate = trial.suggest_float('dropout_rate', 0.05, 0.2)\n    window_size = trial.suggest_categorical('window_size', [12, 32, 64])\n    stride = trial.suggest_categorical('stride', [1])\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n    learning_rate = trial.suggest_float('learning_rate', 0.0003, 0.002, log=True)\n    optimizer = trial.suggest_categorical('optimizer', ['nadam'])\n    num_layers = trial.suggest_int('num_layers', 1, 3)\n    epochs = trial.suggest_int('epochs', 80, 150)\n    X_train, y_train = make_lstm_dataset(train_df, feature_cols, window_size, stride, target_col=rul_col, clip_value=min_irul)\n    X_val, y_val = make_lstm_dataset(val_df, feature_cols, window_size, stride, target_col=rul_col, clip_value=min_irul)\n\n    model = build_model(\n        input_shape=(window_size, X_train.shape[2]),\n        lstm_units=lstm_units,\n        dropout_rate=dropout_rate,\n        learning_rate=learning_rate,\n        optimizer_name=optimizer,\n        num_layers=num_layers\n    )\n    early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=0)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=0)\n    pruning_callback = TFKerasPruningCallback(trial, 'val_loss')\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=epochs,\n        batch_size=batch_size,\n        callbacks=[early_stop, reduce_lr, pruning_callback],\n        verbose=0\n    )\n\n    val_rmse = model.evaluate(X_val, y_val, verbose=0)[1]\n    y_pred = model.predict(X_val, verbose=0).flatten()\n    val_score = phm08_score(y_val, y_pred)\n\n    trial.set_user_attr(\"VAL_RMSE\", val_rmse)\n    trial.set_user_attr(\"VAL_SCORE\", val_score)\n\n    return val_rmse\n\n# ====== 5. åŸ·è¡Œ Optuna æœå°‹æœ€ä½³è¶…åƒæ•¸ ======\nstudy = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED))\nstudy.optimize(objective, n_trials=100, timeout=21600)\n\nprint(\"âœ… Optuna æœå°‹å®Œç•¢ï¼\")\nprint(\"Best params:\", study.best_trial.params)\nprint(\"Best RMSE:\", study.best_value)\n\n# å„²å­˜çµæœ\noptuna_df = study.trials_dataframe()\noptuna_df.to_csv(RESULT_CSV, index=False)\n\n# ====== 6. ä»¥æœ€ä½³è¶…åƒæ•¸å†è¨“ç·´å®Œæ•´æ¨¡å‹ï¼ˆåŒæ¨£åˆ†å‰²ï¼‰======\nbest_params = study.best_trial.params\nnp.random.seed(best_params['seed'])\ntf.random.set_seed(best_params['seed'])\n\nX_train, y_train = make_lstm_dataset(train_df, feature_cols, best_params['window_size'], best_params['stride'], target_col=rul_col, clip_value=min_irul)\nX_val, y_val = make_lstm_dataset(val_df, feature_cols, best_params['window_size'], best_params['stride'], target_col=rul_col, clip_value=min_irul)\n\nbest_model = build_model(\n    input_shape=(best_params['window_size'], X_train.shape[2]),\n    lstm_units=best_params['lstm_units'],\n    dropout_rate=best_params['dropout_rate'],\n    learning_rate=best_params['learning_rate'],\n    optimizer_name=best_params['optimizer'],\n    num_layers=best_params['num_layers']\n)\nbest_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=best_params['epochs'],\n    batch_size=best_params['batch_size'],\n    callbacks=[\n        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n    ],\n    verbose=1\n)\nbest_model.save(MODEL_PATH)\nprint(f\"ğŸ“ æœ€ä½³æ¨¡å‹å„²å­˜æ–¼ï¼š{MODEL_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:54:53.312963Z","iopub.execute_input":"2025-05-31T13:54:53.313285Z","iopub.status.idle":"2025-05-31T13:58:02.770490Z","shell.execute_reply.started":"2025-05-31T13:54:53.313261Z","shell.execute_reply":"2025-05-31T13:58:02.769703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# æ¸¬è©¦é›†è³‡æ–™è™•ç†","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\nimport os\n\nMODEL_DIR = '/kaggle/working'\n\n# === 1. å–å¾—æœ€ä½³SEQ_LEN ===\noptuna_df = pd.read_csv(f'{MODEL_DIR}/fd001_optuna_search_results.csv')\nsort_col = 'user_attrs_VAL_RMSE' if 'user_attrs_VAL_RMSE' in optuna_df.columns else 'value'\nbest_trial = optuna_df.loc[optuna_df[sort_col].idxmin()]\nSEQ_LEN = int(best_trial['params_window_size'])\n\nprint(f\"âœ… æœ€ä½³ SEQ_LEN = {SEQ_LEN}\")\n\n# === 2. è¼‰å…¥å‰è™•ç†è³‡æ–™ ===\nfeature_names = joblib.load(f'{MODEL_DIR}/feature_names.pkl')\nscaler = joblib.load(f'{MODEL_DIR}/scaler_preprocessed.joblib')\n\n# === 3. è®€åŸå§‹test set ===\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\ntest = pd.read_csv('/kaggle/input/cmapssdata/test_FD001.txt', sep='\\s+', header=None)\ntest.columns = columns\nrul_truth = pd.read_csv('/kaggle/input/cmapssdata/RUL_FD001.txt', header=None, names=['true_RUL'])\n\n# === 4. ç‰¹å¾µå·¥ç¨‹ ===\ndef multi_exponential_smoothing(series, alphas=[0.1, 0.3]):\n    results = []\n    for alpha in alphas:\n        smoothed = [series.iloc[0]]\n        for n in range(1, len(series)):\n            smoothed.append(alpha * series.iloc[n] + (1 - alpha) * smoothed[-1])\n        results.append(pd.Series(smoothed, index=series.index))\n    return sum(results) / len(results)\n\nfor col in [c for c in feature_names if not c.endswith('_diff') and c not in ['unit','time']]:\n    test[col] = test.groupby('unit')[col].transform(lambda x: multi_exponential_smoothing(x))\n\nfor col in [c for c in feature_names if c.endswith('_diff')]:\n    base_col = col.replace('_diff', '')\n    test[col] = test.groupby('unit')[base_col].diff().fillna(0)\n\ntest[feature_names] = scaler.transform(test[feature_names])\n\n# === 5. æ»‘å‹•çª—å£çµ„è£ ===\nX_test, unit_ids = [], []\nfor unit in sorted(test['unit'].unique()):\n    df_unit = test[test['unit'] == unit]\n    arr = df_unit[feature_names].values\n    if len(arr) >= SEQ_LEN:\n        X_test.append(arr[-SEQ_LEN:])\n        unit_ids.append(unit)\nX_test = np.stack(X_test).astype(np.float32)\ny_true = rul_truth.loc[np.array(unit_ids) - 1, 'true_RUL'].values\n\nnp.save(f'{MODEL_DIR}/X_test.npy', X_test)\nnp.save(f'{MODEL_DIR}/unit_ids.npy', np.array(unit_ids))\nnp.save(f'{MODEL_DIR}/y_true.npy', y_true)\n\nprint(f\"âœ… æ¸¬è©¦é›†é è™•ç†å®Œæˆ X_test.shape={X_test.shape}ï¼Œunitæ•¸é‡={len(unit_ids)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:58:02.772098Z","iopub.execute_input":"2025-05-31T13:58:02.772561Z","iopub.status.idle":"2025-05-31T13:58:04.559591Z","shell.execute_reply.started":"2025-05-31T13:58:02.772543Z","shell.execute_reply":"2025-05-31T13:58:04.558978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FD001è³‡æ–™é©—è­‰","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nimport random\nimport os\n\nMODEL_DIR = '/kaggle/working'\n\n# ====== å…¨å±€ SEED è¨­å®š ======\nGLOBAL_SEED = 42\nrandom.seed(GLOBAL_SEED)\nnp.random.seed(GLOBAL_SEED)\ntf.random.set_seed(GLOBAL_SEED)\n\n# === PHM08 Score Function ===\ndef phm08_score(y_true, y_pred):\n    error = y_pred - y_true\n    score = np.where(error < 0,\n                     np.exp(-error / 13) - 1,\n                     np.exp(error / 10) - 1)\n    return np.sum(score)\n\n# === (A) ç‰¹å¾µå·¥ç¨‹ï¼ˆå–®å° engine ä¹Ÿå¯è¤‡ç”¨ï¼‰ ===\ndef multi_exponential_smoothing(series, alphas=[0.1, 0.3]):\n    results = []\n    for alpha in alphas:\n        smoothed = [series.iloc[0]]\n        for n in range(1, len(series)):\n            smoothed.append(alpha * series.iloc[n] + (1 - alpha) * smoothed[-1])\n        results.append(pd.Series(smoothed, index=series.index))\n    return sum(results) / len(results)\n\ndef preprocess_for_engine(df, feature_names, scaler):\n    # Exponential smoothing\n    for col in [c for c in feature_names if not c.endswith('_diff') and c not in ['unit','time']]:\n        df[col] = multi_exponential_smoothing(df[col])\n    # Diff ç‰¹å¾µ\n    for col in [c for c in feature_names if c.endswith('_diff')]:\n        base_col = col.replace('_diff', '')\n        df[col] = df[base_col].diff().fillna(0)\n    # æ¨™æº–åŒ–\n    df[feature_names] = scaler.transform(df[feature_names])\n    return df\n\n# === 1. è¼‰å…¥é è™•ç†å¾Œæ¸¬è©¦é›† ===\nX_test = np.load(f'{MODEL_DIR}/X_test.npy')\nunit_ids = np.load(f'{MODEL_DIR}/unit_ids.npy')\ny_true = np.load(f'{MODEL_DIR}/y_true.npy')\n\n# === 2. è¼‰å…¥æ¨¡å‹èˆ‡ç‰¹å¾µå ===\nMODEL_PATH = f'{MODEL_DIR}/best_fd001_lstm_model_mse.keras'\nfeature_cols = joblib.load(f'{MODEL_DIR}/feature_names.pkl')\nscaler = joblib.load(f'{MODEL_DIR}/scaler_preprocessed.joblib')\nmodel = tf.keras.models.load_model(MODEL_PATH)\n\n# === 3. è‡ªå‹•æŠ“å–æœ€ä½³ SEQ_LENï¼ˆç”¨æ–¼ç•«å–®ä¸€ engineï¼‰===\noptuna_df = pd.read_csv(f'{MODEL_DIR}/fd001_optuna_search_results.csv')\nsort_col = 'user_attrs_VAL_RMSE' if 'user_attrs_VAL_RMSE' in optuna_df.columns else 'value'\nbest_trial = optuna_df.loc[optuna_df[sort_col].idxmin()]\nSEQ_LEN = int(best_trial['params_window_size'])\n\n# === 4. clip ä¸Šé™ ===\ntry:\n    clip_upper = int(np.load(f'{MODEL_DIR}/fd001_min_irul.npy'))\n    print(f\"âœ… è‡ªå‹• clip ä¸Šé™ï¼š{clip_upper}\")\nexcept Exception as e:\n    clip_upper = 130\n    print(f\"âš ï¸ æ‰¾ä¸åˆ° min_irulï¼Œé è¨­ clip ä¸Šé™ï¼š{clip_upper} ({e})\")\n\n# === 5. é æ¸¬èˆ‡è©•åˆ† ===\ny_pred = model.predict(X_test, verbose=0).flatten()\ny_pred = np.clip(y_pred, 0, clip_upper)\n\nrmse = np.sqrt(mean_squared_error(y_true, y_pred))\nscore = phm08_score(y_true, y_pred)\n\ndf_out = pd.DataFrame({\n    'engine_id': unit_ids,\n    'true_RUL': y_true,\n    'predicted_RUL': y_pred,\n    'error': y_pred - y_true\n})\ndf_out['score_component'] = np.where(df_out['error'] < 0,\n                                     np.exp(-df_out['error'] / 13) - 1,\n                                     np.exp(df_out['error'] / 10) - 1)\ndf_out['RMSE'] = rmse\ndf_out['Score'] = score\ndf_out.to_csv(f'{MODEL_DIR}/fd001_test_rul_results.csv', index=False)\n\n# === (a) çœŸå¯¦vsé æ¸¬RUL ===\nplt.figure(figsize=(12, 6))\nplt.plot(df_out['engine_id'], df_out['true_RUL'], label='True RUL', marker='o')\nplt.plot(df_out['engine_id'], df_out['predicted_RUL'], label='Predicted RUL', marker='x')\nplt.title(f'FD001: True vs Predicted RUL\\nRMSE={rmse:.2f} | Score={score:.2f}')\nplt.xlabel('Engine ID')\nplt.ylabel('RUL')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd001_test_rul_plot.png')\nplt.close()\n\n# === (b) é æ¸¬èª¤å·®ç›´æ–¹åœ– ===\nplt.figure(figsize=(10, 5))\nplt.hist(df_out['error'], bins=25, color='skyblue', edgecolor='black')\nplt.title('FD001 Prediction Error Histogram')\nplt.xlabel('Prediction Error (Predicted - True)')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd001_error_histogram.png')\nplt.close()\n\n# === (c) æ®˜å·®åˆ†å¸ƒåœ–ï¼ˆError vs True RUL scatterï¼‰===\nplt.figure(figsize=(10, 6))\nplt.scatter(df_out['true_RUL'], df_out['error'], c='royalblue', alpha=0.7, edgecolors='k')\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('True RUL')\nplt.ylabel('Prediction Error (Predicted - True)')\nplt.title('Residual Distribution (Error vs True RUL)')\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd001_residual_scatter.png')\nplt.close()\n\n# === (d) å–®ä¸€ engine ç”Ÿå‘½é€±æœŸé æ¸¬æ›²ç·šï¼ˆæœ€å¤§èª¤å·® engineï¼‰===\nSAMPLE_ENGINE_ID = int(df_out.iloc[df_out['error'].abs().argmax()]['engine_id'])\n\n# è®€åŸå§‹ test rawï¼Œåšå‰è™•ç†ï¼ˆ**å¿…é ˆï¼**ï¼‰\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\ntest_raw = pd.read_csv('/kaggle/input/cmapssdata/test_FD001.txt', sep='\\s+', header=None)\ntest_raw.columns = columns\nsample_df = test_raw[test_raw['unit'] == SAMPLE_ENGINE_ID].sort_values('time').reset_index(drop=True)\n\n# åŠ å…¥ç‰¹å¾µå·¥ç¨‹ + scalerï¼ˆä¿æŒå’Œè¨“ç·´ä¸€è‡´ï¼‰\nsample_df = preprocess_for_engine(sample_df, feature_cols, scaler)\nsample_features = sample_df[feature_cols].values\n\nsample_preds = []\nfor i in range(SEQ_LEN, len(sample_df) + 1):\n    seq_x = sample_features[i-SEQ_LEN:i]\n    pred = model.predict(seq_x[np.newaxis, :, :], verbose=0).flatten()[0]\n    sample_preds.append(pred)\nsample_ruls = np.arange(len(sample_df)-1, -1, -1)[:len(sample_preds)]  # å°é½Šé•·åº¦\n\nplt.figure(figsize=(12, 6))\nplt.plot(range(len(sample_preds)), sample_preds, label='Predicted RUL', marker='x')\nplt.plot(range(len(sample_ruls)), sample_ruls, label='True RUL', marker='o')\nplt.title(f'Engine {SAMPLE_ENGINE_ID}: RUL Prediction (Full Cycle)')\nplt.xlabel('Cycle')\nplt.ylabel('RUL')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd001_sample_engine_curve.png')\nplt.close()\n\nprint(f\"\\nâœ… æ¸¬è©¦é›† RMSE = {rmse:.4f}\")\nprint(f\"âœ… æ¸¬è©¦é›† Score = {score:.4f}\")\nprint(f\"ğŸ“Š æœ€å¤§èª¤å·®ï¼š{df_out['error'].max():.2f}\")\nprint(f\"ğŸ“Š æœ€å°èª¤å·®ï¼š{df_out['error'].min():.2f}\")\nprint(f\"ğŸ“Š è¶…é Â±100 é æ¸¬æ•¸é‡ï¼š{np.sum(np.abs(df_out['error']) > 100)}\")\nprint(f\"ğŸ“ˆ åœ–ç‰‡å·²å„²å­˜ï¼š\")\nprint(f\"{MODEL_DIR}/fd001_test_rul_plot.png\")\nprint(f\"{MODEL_DIR}/fd001_error_histogram.png\")\nprint(f\"{MODEL_DIR}/fd001_residual_scatter.png\")\nprint(f\"{MODEL_DIR}/fd001_sample_engine_curve.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:58:04.560615Z","iopub.execute_input":"2025-05-31T13:58:04.561668Z","iopub.status.idle":"2025-05-31T13:58:08.771007Z","shell.execute_reply.started":"2025-05-31T13:58:04.561643Z","shell.execute_reply":"2025-05-31T13:58:08.770360Z"}},"outputs":[],"execution_count":null}]}