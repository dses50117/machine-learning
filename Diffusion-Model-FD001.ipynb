{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "安裝必要套件"
      ],
      "metadata": {
        "id": "PP5K_qNURO0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow scikit-learn pandas matplotlib tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvlcTJ4ZRSG3",
        "outputId": "eafd99aa-23ea-4a47-e3ae-eff0cc98e065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "掛載 Google Drive"
      ],
      "metadata": {
        "id": "eZ3niuUTRVee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 設定雲端硬碟目錄\n",
        "import os\n",
        "MODEL_DIR = '/content/drive/MyDrive/rul_diffusion_fd001'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qBs-ty8RWCZ",
        "outputId": "53debd85-6bf1-45ff-d627-657e93f78a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna optuna-integration[tfkeras] -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqxTh3NHseRS",
        "outputId": "52fbe422-03fa-4cf5-f572-9ab5738367fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/242.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/98.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvXWAI5xy0qC",
        "outputId": "58d298ef-3b18-46ca-e335-096461c6919f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApndclL5y1Fi",
        "outputId": "7ea2aca0-d14d-4e05-ca0c-681cd65f071d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow scikit-learn matplotlib python-docx -q"
      ],
      "metadata": {
        "id": "G7-7QpIPywap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFhfBDTcnixO",
        "outputId": "13517bf2-78b5-4338-c150-90963d0189da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm5X6pYenn4n",
        "outputId": "7e941545-cf1e-417e-f42d-fa0ac54f045f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPA8IqHk3E4i",
        "outputId": "eeea80e2-bb8c-47d3-b6e1-4d25f879dc3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow scikit-learn pandas matplotlib tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下載並載入 C-MAPSS 資料集（FD001)"
      ],
      "metadata": {
        "id": "9_iZz43s3iVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 上傳 kaggle.json（只需做一次）\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "LW1B1ufJ3ZWV",
        "outputId": "18bf4a24-0d1a-4a9c-b0ca-b3afe86d365b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0b31950b-269f-4356-ad26-a87375db1e84\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0b31950b-269f-4356-ad26-a87375db1e84\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"billlin01\",\"key\":\"c42c112425b5d48a2926ec740846bfc2\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "設定 Kaggle API 權限並下載資料"
      ],
      "metadata": {
        "id": "jiUVyr3w7EkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "!cp kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# 以 NASA C-MAPSS 資料為例\n",
        "# 替換成你要的 Kaggle dataset slug（網址最後一段）\n",
        "!kaggle datasets download -d behrad3d/nasa-cmaps\n",
        "!unzip -o nasa-cmaps.zip -d cmapss_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8PPOi867E5m",
        "outputId": "0d5c1ec0-42c8-461b-c474-7d800cdaf885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/behrad3d/nasa-cmaps\n",
            "License(s): CC0-1.0\n",
            "Downloading nasa-cmaps.zip to /content\n",
            "  0% 0.00/12.3M [00:00<?, ?B/s]\n",
            "100% 12.3M/12.3M [00:00<00:00, 856MB/s]\n",
            "Archive:  nasa-cmaps.zip\n",
            "  inflating: cmapss_data/CMaps/Damage Propagation Modeling.pdf  \n",
            "  inflating: cmapss_data/CMaps/RUL_FD001.txt  \n",
            "  inflating: cmapss_data/CMaps/RUL_FD002.txt  \n",
            "  inflating: cmapss_data/CMaps/RUL_FD003.txt  \n",
            "  inflating: cmapss_data/CMaps/RUL_FD004.txt  \n",
            "  inflating: cmapss_data/CMaps/readme.txt  \n",
            "  inflating: cmapss_data/CMaps/test_FD001.txt  \n",
            "  inflating: cmapss_data/CMaps/test_FD002.txt  \n",
            "  inflating: cmapss_data/CMaps/test_FD003.txt  \n",
            "  inflating: cmapss_data/CMaps/test_FD004.txt  \n",
            "  inflating: cmapss_data/CMaps/train_FD001.txt  \n",
            "  inflating: cmapss_data/CMaps/train_FD002.txt  \n",
            "  inflating: cmapss_data/CMaps/train_FD003.txt  \n",
            "  inflating: cmapss_data/CMaps/train_FD004.txt  \n",
            "  inflating: cmapss_data/CMaps/x.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "讀取資料夾路徑"
      ],
      "metadata": {
        "id": "-_T74oCO6cR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('/content/cmapss_data/CMaps/train_FD001.txt', sep='\\s+', header=None)\n",
        "print(train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQPbr2oL6cmg",
        "outputId": "b600cfa7-2b8a-44c3-c077-4586282edf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0   1       2       3      4       5       6        7        8      9   \\\n",
            "0   1   1 -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60  14.62   \n",
            "1   1   2  0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14  14.62   \n",
            "2   1   3 -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20  14.62   \n",
            "3   1   4  0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87  14.62   \n",
            "4   1   5 -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22  14.62   \n",
            "\n",
            "   ...      16       17       18      19    20   21    22     23     24  \\\n",
            "0  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388  100.0  39.06   \n",
            "1  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388  100.0  39.00   \n",
            "2  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388  100.0  38.95   \n",
            "3  ...  522.86  2388.08  8133.83  8.3682  0.03  392  2388  100.0  38.88   \n",
            "4  ...  522.19  2388.04  8133.80  8.4294  0.03  393  2388  100.0  38.90   \n",
            "\n",
            "        25  \n",
            "0  23.4190  \n",
            "1  23.4236  \n",
            "2  23.3442  \n",
            "3  23.3739  \n",
            "4  23.4044  \n",
            "\n",
            "[5 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "資料預處理（選感測器、平滑、標準化並存scaler）"
      ],
      "metadata": {
        "id": "gHCteQuZ3osC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "\n",
        "GLOBAL_SEED = 42\n",
        "random.seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "MODEL_DIR = '/content/drive/MyDrive/rul_diffusion_fd001'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "max_rul = 130\n",
        "\n",
        "columns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\n",
        "\n",
        "def load_and_label(filepath, max_rul=130):\n",
        "    df = pd.read_csv(filepath, sep='\\s+', header=None)\n",
        "    df.columns = columns\n",
        "    df['RUL_linear'] = df.groupby('unit')['time'].transform('max') - df['time']\n",
        "    df['RUL_linear'] = df['RUL_linear'].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "train = load_and_label('/content/cmapss_data/CMaps/train_FD001.txt', max_rul=max_rul)\n",
        "\n",
        "# ---- 1. 挑掉變異太低的sensor ----\n",
        "sensor_cols = [f's{i}' for i in range(1, 22)]\n",
        "sensor_cols_keep = [col for col in sensor_cols if train[col].std() >= 1e-3]  # 防止有無變異sensor\n",
        "\n",
        "# ---- 2. 分段線性退化 RUL標註 (你的 Algorithm1 不變) ----\n",
        "def generate_piecewise_rul_algorithm1(df, feature_cols, window_size=12, threshold=0.2, max_rul=130, verbose=False):\n",
        "    result_dfs = []\n",
        "    for unit_id, group in df.groupby('unit'):\n",
        "        group = group.sort_values('time').reset_index(drop=True)\n",
        "        sensor_data = group[feature_cols].values\n",
        "        num_cycles = len(group)\n",
        "        num_windows = num_cycles // window_size\n",
        "\n",
        "        if num_windows < 3:\n",
        "            irul = num_cycles\n",
        "            group['iRUL'] = irul\n",
        "            group['RUL_piecewise'] = np.clip(irul - group['time'], 0, max_rul)\n",
        "            result_dfs.append(group)\n",
        "            continue\n",
        "\n",
        "        centroids = [np.mean(sensor_data[i * window_size:(i + 1) * window_size], axis=0)\n",
        "                     for i in range(num_windows)]\n",
        "        base = centroids[0]  # 第一段健康期\n",
        "        degradation_found = False\n",
        "\n",
        "        for i in range(2, num_windows):  # 從 w3 開始比對\n",
        "            dist_sq = np.sum((centroids[i] - base) ** 2)\n",
        "            if verbose:\n",
        "                print(f\"[Unit {unit_id}] Compare w1 to w{i+1}: Dist^2 = {dist_sq:.4f}\")\n",
        "            if dist_sq >= threshold:\n",
        "                degradation_start = i * window_size\n",
        "                irul = num_cycles - degradation_start\n",
        "                degradation_found = True\n",
        "                break\n",
        "\n",
        "        if not degradation_found:\n",
        "            irul = num_cycles\n",
        "\n",
        "        rul_piecewise = [\n",
        "            irul if t <= (num_cycles - irul) else irul - (t - (num_cycles - irul))\n",
        "            for t in range(num_cycles)\n",
        "        ]\n",
        "        group['iRUL'] = irul\n",
        "        group['RUL_piecewise'] = np.clip(rul_piecewise, 0, max_rul)\n",
        "        result_dfs.append(group)\n",
        "    return pd.concat(result_dfs, ignore_index=True)\n",
        "\n",
        "train = generate_piecewise_rul_algorithm1(\n",
        "    train, sensor_cols_keep, window_size=12, threshold=0.2, max_rul=max_rul, verbose=False\n",
        ")\n",
        "\n",
        "# ---- 3. SHAP + 皮爾森選sensor (保證只選8個) ----\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED)\n",
        "rf.fit(train[sensor_cols_keep], train['RUL_piecewise'])\n",
        "\n",
        "explainer = shap.Explainer(rf, train[sensor_cols_keep].iloc[:500])\n",
        "shap_values = explainer(train[sensor_cols_keep].iloc[:500])\n",
        "mean_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "shap_scores = pd.Series(mean_shap, index=sensor_cols_keep).sort_values(ascending=False)\n",
        "top8_shap = shap_scores.head(8).index.tolist()\n",
        "\n",
        "corrs = train[sensor_cols_keep + ['RUL_piecewise']].corr()['RUL_piecewise'].abs().sort_values(ascending=False)\n",
        "top8_pearson = corrs.drop('RUL_piecewise').head(8).index.tolist()\n",
        "\n",
        "selected_sensors = []\n",
        "for s in top8_shap:\n",
        "    if s in top8_pearson and s not in selected_sensors:\n",
        "        selected_sensors.append(s)\n",
        "for s in top8_pearson:\n",
        "    if s not in selected_sensors:\n",
        "        selected_sensors.append(s)\n",
        "    if len(selected_sensors) == 8:\n",
        "        break\n",
        "print(\"最終選用的8個感測器:\", selected_sensors)\n",
        "\n",
        "features = selected_sensors + ['op1', 'op2', 'op3']\n",
        "\n",
        "# ---- 4. 單一指數平滑 (alpha=0.2) ----\n",
        "def exponential_smoothing(series, alpha=0.2):\n",
        "    result = [series.iloc[0]]\n",
        "    for n in range(1, len(series)):\n",
        "        result.append(alpha * series.iloc[n] + (1 - alpha) * result[-1])\n",
        "    return pd.Series(result, index=series.index)\n",
        "\n",
        "for col in features:\n",
        "    train[col] = train.groupby('unit')[col].transform(lambda x: exponential_smoothing(x, alpha=0.2))\n",
        "\n",
        "final_features = features\n",
        "\n",
        "# ---- 5. 標準化 ----\n",
        "scaler = StandardScaler()\n",
        "train[final_features] = scaler.fit_transform(train[final_features])\n",
        "\n",
        "# ---- 6. 儲存 ----\n",
        "joblib.dump(scaler, f'{MODEL_DIR}/scaler_preprocessed.joblib')\n",
        "joblib.dump(final_features, f'{MODEL_DIR}/feature_names.pkl')\n",
        "train.to_csv(f'{MODEL_DIR}/train_with_piecewise_rul.csv', index=False)\n",
        "\n",
        "print('🚩 已完成分段線性RUL資料與所有特徵處理，檔案儲存：')\n",
        "print(f'- {MODEL_DIR}/train_with_piecewise_rul.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omBUOvkY_B1h",
        "outputId": "a1116533-fb21-4fe3-c436-a6e8c6246f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|===================| 497/500 [01:02<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最終選用的8個感測器: ['s11', 's4', 's12', 's7', 's21', 's15', 's20', 's17']\n",
            "🚩 已完成分段線性RUL資料與所有特徵處理，檔案儲存：\n",
            "- /content/drive/MyDrive/rul_diffusion_fd001/train_with_piecewise_rul.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "產生 X_train 滑動視窗序列"
      ],
      "metadata": {
        "id": "SjcVoaJaM7I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# 參數設定\n",
        "SEQ_LEN = 32\n",
        "MODEL_DIR = '/content/drive/MyDrive/rul_diffusion_fd001'\n",
        "\n",
        "# 載入先前前處理後的特徵名稱\n",
        "final_features = joblib.load(f'{MODEL_DIR}/feature_names.pkl')\n",
        "\n",
        "# 載入資料（和前處理結果保持一致）\n",
        "train = pd.read_csv(f'{MODEL_DIR}/train_with_piecewise_rul.csv')\n",
        "\n",
        "# 建構滑動序列\n",
        "X_list, y_list, unit_list = [], [], []\n",
        "for unit in train['unit'].unique():\n",
        "    df_unit = train[train['unit'] == unit]\n",
        "    arr = df_unit[final_features].values\n",
        "    rul_piecewise = df_unit['RUL_piecewise'].values  # label用piecewise\n",
        "    # 產生滑動視窗序列\n",
        "    for i in range(len(arr) - SEQ_LEN + 1):\n",
        "        X_list.append(arr[i:i + SEQ_LEN])\n",
        "        y_list.append(rul_piecewise[i + SEQ_LEN - 1])\n",
        "        unit_list.append(unit)\n",
        "\n",
        "X_all = np.stack(X_list).astype(np.float32)\n",
        "y_all = np.array(y_list).astype(np.float32)\n",
        "unit_all = np.array(unit_list).astype(int)\n",
        "\n",
        "print(\"滑動視窗完成:\")\n",
        "print(\"X_all shape:\", X_all.shape)  # (N, 32, 11)\n",
        "print(\"y_all shape:\", y_all.shape)  # (N,)\n",
        "\n",
        "# 儲存資料（供後續encoder/joint train用）\n",
        "np.save(f'{MODEL_DIR}/X_all.npy', X_all)\n",
        "np.save(f'{MODEL_DIR}/y_all.npy', y_all)\n",
        "np.save(f'{MODEL_DIR}/unit_all.npy', unit_all)  # optional, 分組/val用\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e91GBMTH_MRO",
        "outputId": "ea3a3cb5-5288-45b1-85ff-dad02c9d2bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "滑動視窗完成:\n",
            "X_all shape: (17531, 32, 11)\n",
            "y_all shape: (17531,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stage 1 Encoder自監督預訓練，儲存模型到雲端"
      ],
      "metadata": {
        "id": "SABfv4cH4bqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Input, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os, random\n",
        "\n",
        "GLOBAL_SEED = 42\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "tf.random.set_seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "SEQ_LEN = 32\n",
        "D = 16\n",
        "T = 1000\n",
        "MODEL_DIR = '/content/drive/MyDrive/rul_diffusion_fd001'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "N_SENSORS = 11  # 你的特徵數\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS_STAGE1 = 300\n",
        "\n",
        "# Stage1完整模型（輸出復原序列）\n",
        "def build_encoder_full(input_dim, embed_dim, seq_len):\n",
        "    input_x = Input(shape=(seq_len, input_dim), name='input_1')\n",
        "    input_tau = Input(shape=(embed_dim,), name='input_2')\n",
        "    t_proj = layers.Dense(input_dim)(input_tau)\n",
        "    t_proj_exp = layers.Reshape((1, input_dim))(t_proj)\n",
        "    x = layers.Add()([input_x, t_proj_exp])\n",
        "    x = layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    output = layers.Conv1D(input_dim, 1, padding='same')(x)  # 預測噪音序列\n",
        "    return Model([input_x, input_tau], output)\n",
        "\n",
        "encoder_full = build_encoder_full(N_SENSORS, D, SEQ_LEN)\n",
        "encoder_full.compile(optimizer=Adam(learning_rate=5e-4), loss='mse')\n",
        "\n",
        "# Diffusion Beta schedule (cosine)\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = np.linspace(0, timesteps, steps)\n",
        "    f = np.cos(((x / timesteps) + s) / (1 + s) * np.pi / 2) ** 2\n",
        "    alphas_cumprod = f / f[0]\n",
        "    betas = np.clip(1 - (alphas_cumprod[1:] / alphas_cumprod[:-1]), 0, 0.999)\n",
        "    return betas\n",
        "\n",
        "betas = cosine_beta_schedule(T, s=0.008)\n",
        "alphas = 1 - betas\n",
        "alphas_cumprod = np.cumprod(alphas)\n",
        "\n",
        "# 時間嵌入函數\n",
        "def get_timestep_embedding(timesteps, dim=D):\n",
        "    timesteps = tf.convert_to_tensor(timesteps, dtype=tf.float32)\n",
        "    timesteps = tf.reshape(timesteps, [-1, 1])\n",
        "    half_dim = dim // 2\n",
        "    emb = tf.math.log(10000.0) / (half_dim - 1)\n",
        "    emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n",
        "    emb = timesteps * emb\n",
        "    emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n",
        "    return emb\n",
        "\n",
        "# Diffusion資料生成器\n",
        "def diffusion_dataset(X, batch_size, T, alphas_cumprod, D):\n",
        "    def generator():\n",
        "        dataset_size = len(X)\n",
        "        while True:\n",
        "            idxs = np.random.permutation(dataset_size)\n",
        "            for i in range(0, dataset_size, batch_size):\n",
        "                batch_idx = idxs[i:i+batch_size]\n",
        "                x_start = X[batch_idx]\n",
        "                b = len(x_start)\n",
        "                t = np.random.randint(1, T + 1, size=b)\n",
        "                tau = get_timestep_embedding(t, D).numpy()\n",
        "                noise = np.random.randn(*x_start.shape).astype(np.float32)\n",
        "                sqrt_alpha = np.sqrt(alphas_cumprod[t - 1])[:, None, None]\n",
        "                sqrt_one_minus_alpha = np.sqrt(1 - alphas_cumprod[t - 1])[:, None, None]\n",
        "                x_t = sqrt_alpha * x_start + sqrt_one_minus_alpha * noise\n",
        "                yield {\"input_1\": x_t, \"input_2\": tau}, noise\n",
        "    output_signature = (\n",
        "        {\"input_1\": tf.TensorSpec(shape=(None, SEQ_LEN, N_SENSORS), dtype=tf.float32),\n",
        "         \"input_2\": tf.TensorSpec(shape=(None, D), dtype=tf.float32)},\n",
        "        tf.TensorSpec(shape=(None, SEQ_LEN, N_SENSORS), dtype=tf.float32)\n",
        "    )\n",
        "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# 載入資料 (X_all需先製作好，shape: (N, 32, 11))\n",
        "X_all = np.load(f'{MODEL_DIR}/X_all.npy')\n",
        "\n",
        "train_ds = diffusion_dataset(X_all, BATCH_SIZE, T, alphas_cumprod, D)\n",
        "steps_per_epoch = len(X_all) // BATCH_SIZE\n",
        "\n",
        "# 訓練 Stage1 預訓練模型\n",
        "history = encoder_full.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS_STAGE1,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "# 建立embedding模型，取倒數第二Conv1D層輸出後GlobalAveragePooling1D\n",
        "embedding_layer_name = encoder_full.layers[-2].name\n",
        "embedding_output = encoder_full.get_layer(embedding_layer_name).output\n",
        "embedding_gap = layers.GlobalAveragePooling1D()(embedding_output)\n",
        "encoder_embedding = Model(encoder_full.inputs, embedding_gap)\n",
        "\n",
        "# 儲存模型，建議用.keras格式避免legacy warning\n",
        "encoder_full.save(f'{MODEL_DIR}/stage1_encoder_full.keras')\n",
        "encoder_embedding.save(f'{MODEL_DIR}/stage1_encoder_embedding.keras')\n",
        "\n",
        "print(\"✅ Stage1 預訓練完成，模型與embedding模型已儲存。\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOUwUoFn_psz",
        "outputId": "808013f3-17e2-48e8-fcd7-6fc09c12554b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.8330\n",
            "Epoch 2/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3345\n",
            "Epoch 3/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3018\n",
            "Epoch 4/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2804\n",
            "Epoch 5/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2722\n",
            "Epoch 6/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2562\n",
            "Epoch 7/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2386\n",
            "Epoch 8/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2275\n",
            "Epoch 9/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2273\n",
            "Epoch 10/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2154\n",
            "Epoch 11/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2163\n",
            "Epoch 12/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2185\n",
            "Epoch 13/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2191\n",
            "Epoch 14/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2126\n",
            "Epoch 15/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2112\n",
            "Epoch 16/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2097\n",
            "Epoch 17/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2116\n",
            "Epoch 18/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2127\n",
            "Epoch 19/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2031\n",
            "Epoch 20/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2026\n",
            "Epoch 21/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2056\n",
            "Epoch 22/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2061\n",
            "Epoch 23/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2035\n",
            "Epoch 24/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2041\n",
            "Epoch 25/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2014\n",
            "Epoch 26/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2013\n",
            "Epoch 27/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2032\n",
            "Epoch 28/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1994\n",
            "Epoch 29/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1984\n",
            "Epoch 30/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2000\n",
            "Epoch 31/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1987\n",
            "Epoch 32/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2026\n",
            "Epoch 33/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2010\n",
            "Epoch 34/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1976\n",
            "Epoch 35/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2009\n",
            "Epoch 36/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1985\n",
            "Epoch 37/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1977\n",
            "Epoch 38/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1960\n",
            "Epoch 39/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1984\n",
            "Epoch 40/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1951\n",
            "Epoch 41/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1950\n",
            "Epoch 42/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1967\n",
            "Epoch 43/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1981\n",
            "Epoch 44/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1951\n",
            "Epoch 45/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1991\n",
            "Epoch 46/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1958\n",
            "Epoch 47/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1985\n",
            "Epoch 48/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1965\n",
            "Epoch 49/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1961\n",
            "Epoch 50/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1949\n",
            "Epoch 51/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1946\n",
            "Epoch 52/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1999\n",
            "Epoch 53/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1975\n",
            "Epoch 54/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1952\n",
            "Epoch 55/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1983\n",
            "Epoch 56/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1918\n",
            "Epoch 57/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1933\n",
            "Epoch 58/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1925\n",
            "Epoch 59/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1922\n",
            "Epoch 60/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1961\n",
            "Epoch 61/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1995\n",
            "Epoch 62/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1956\n",
            "Epoch 63/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1947\n",
            "Epoch 64/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1939\n",
            "Epoch 65/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1923\n",
            "Epoch 66/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1955\n",
            "Epoch 67/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1958\n",
            "Epoch 68/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1952\n",
            "Epoch 69/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1919\n",
            "Epoch 70/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1970\n",
            "Epoch 71/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1902\n",
            "Epoch 72/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1949\n",
            "Epoch 73/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1912\n",
            "Epoch 74/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1944\n",
            "Epoch 75/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1921\n",
            "Epoch 76/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1903\n",
            "Epoch 77/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1898\n",
            "Epoch 78/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1935\n",
            "Epoch 79/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1947\n",
            "Epoch 80/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1929\n",
            "Epoch 81/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1894\n",
            "Epoch 82/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1934\n",
            "Epoch 83/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1925\n",
            "Epoch 84/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1940\n",
            "Epoch 85/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1917\n",
            "Epoch 86/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1926\n",
            "Epoch 87/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1906\n",
            "Epoch 88/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1897\n",
            "Epoch 89/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1913\n",
            "Epoch 90/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1917\n",
            "Epoch 91/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1891\n",
            "Epoch 92/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1915\n",
            "Epoch 93/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1900\n",
            "Epoch 94/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1871\n",
            "Epoch 95/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1883\n",
            "Epoch 96/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1915\n",
            "Epoch 97/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1931\n",
            "Epoch 98/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1890\n",
            "Epoch 99/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1882\n",
            "Epoch 100/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1898\n",
            "Epoch 101/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1868\n",
            "Epoch 102/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1911\n",
            "Epoch 103/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1885\n",
            "Epoch 104/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1905\n",
            "Epoch 105/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1912\n",
            "Epoch 106/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1915\n",
            "Epoch 107/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1926\n",
            "Epoch 108/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1927\n",
            "Epoch 109/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1900\n",
            "Epoch 110/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1880\n",
            "Epoch 111/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1882\n",
            "Epoch 112/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1931\n",
            "Epoch 113/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1929\n",
            "Epoch 114/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1887\n",
            "Epoch 115/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1934\n",
            "Epoch 116/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1879\n",
            "Epoch 117/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1889\n",
            "Epoch 118/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1891\n",
            "Epoch 119/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1912\n",
            "Epoch 120/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1923\n",
            "Epoch 121/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1926\n",
            "Epoch 122/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1909\n",
            "Epoch 123/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1908\n",
            "Epoch 124/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1893\n",
            "Epoch 125/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1900\n",
            "Epoch 126/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1936\n",
            "Epoch 127/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1897\n",
            "Epoch 128/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1887\n",
            "Epoch 129/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1899\n",
            "Epoch 130/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1921\n",
            "Epoch 131/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1920\n",
            "Epoch 132/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1918\n",
            "Epoch 133/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1909\n",
            "Epoch 134/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1897\n",
            "Epoch 135/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1891\n",
            "Epoch 136/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1879\n",
            "Epoch 137/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1879\n",
            "Epoch 138/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1888\n",
            "Epoch 139/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1917\n",
            "Epoch 140/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1894\n",
            "Epoch 141/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1886\n",
            "Epoch 142/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1918\n",
            "Epoch 143/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1897\n",
            "Epoch 144/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1870\n",
            "Epoch 145/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1903\n",
            "Epoch 146/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1915\n",
            "Epoch 147/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1881\n",
            "Epoch 148/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1907\n",
            "Epoch 149/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1859\n",
            "Epoch 150/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1888\n",
            "Epoch 151/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1848\n",
            "Epoch 152/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1908\n",
            "Epoch 153/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1884\n",
            "Epoch 154/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1893\n",
            "Epoch 155/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1872\n",
            "Epoch 156/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1862\n",
            "Epoch 157/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1881\n",
            "Epoch 158/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1929\n",
            "Epoch 159/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1877\n",
            "Epoch 160/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1892\n",
            "Epoch 161/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1882\n",
            "Epoch 162/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1842\n",
            "Epoch 163/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1913\n",
            "Epoch 164/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1897\n",
            "Epoch 165/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1858\n",
            "Epoch 166/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1866\n",
            "Epoch 167/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1845\n",
            "Epoch 168/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1862\n",
            "Epoch 169/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1875\n",
            "Epoch 170/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1914\n",
            "Epoch 171/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1872\n",
            "Epoch 172/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1881\n",
            "Epoch 173/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1860\n",
            "Epoch 174/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1907\n",
            "Epoch 175/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1881\n",
            "Epoch 176/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1903\n",
            "Epoch 177/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1895\n",
            "Epoch 178/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1874\n",
            "Epoch 179/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1877\n",
            "Epoch 180/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1872\n",
            "Epoch 181/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1833\n",
            "Epoch 182/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1871\n",
            "Epoch 183/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1828\n",
            "Epoch 184/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1888\n",
            "Epoch 185/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1864\n",
            "Epoch 186/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1848\n",
            "Epoch 187/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1791\n",
            "Epoch 188/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1879\n",
            "Epoch 189/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1894\n",
            "Epoch 190/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1843\n",
            "Epoch 191/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1853\n",
            "Epoch 192/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1883\n",
            "Epoch 193/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1887\n",
            "Epoch 194/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1842\n",
            "Epoch 195/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1876\n",
            "Epoch 196/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1882\n",
            "Epoch 197/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1876\n",
            "Epoch 198/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1865\n",
            "Epoch 199/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1871\n",
            "Epoch 200/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1861\n",
            "Epoch 201/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1867\n",
            "Epoch 202/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1841\n",
            "Epoch 203/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1904\n",
            "Epoch 204/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1897\n",
            "Epoch 205/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1851\n",
            "Epoch 206/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1881\n",
            "Epoch 207/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1811\n",
            "Epoch 208/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1846\n",
            "Epoch 209/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1857\n",
            "Epoch 210/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1819\n",
            "Epoch 211/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1865\n",
            "Epoch 212/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1849\n",
            "Epoch 213/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1842\n",
            "Epoch 214/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1820\n",
            "Epoch 215/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1865\n",
            "Epoch 216/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1844\n",
            "Epoch 217/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1849\n",
            "Epoch 218/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1859\n",
            "Epoch 219/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1820\n",
            "Epoch 220/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1844\n",
            "Epoch 221/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1866\n",
            "Epoch 222/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1832\n",
            "Epoch 223/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1891\n",
            "Epoch 224/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1840\n",
            "Epoch 225/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1846\n",
            "Epoch 226/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1863\n",
            "Epoch 227/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1854\n",
            "Epoch 228/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1856\n",
            "Epoch 229/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1840\n",
            "Epoch 230/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1895\n",
            "Epoch 231/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1850\n",
            "Epoch 232/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1864\n",
            "Epoch 233/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1884\n",
            "Epoch 234/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1896\n",
            "Epoch 235/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1810\n",
            "Epoch 236/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1904\n",
            "Epoch 237/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1875\n",
            "Epoch 238/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1823\n",
            "Epoch 239/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1837\n",
            "Epoch 240/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1849\n",
            "Epoch 241/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1883\n",
            "Epoch 242/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1846\n",
            "Epoch 243/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1847\n",
            "Epoch 244/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1873\n",
            "Epoch 245/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1862\n",
            "Epoch 246/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1855\n",
            "Epoch 247/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1864\n",
            "Epoch 248/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1838\n",
            "Epoch 249/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1810\n",
            "Epoch 250/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1883\n",
            "Epoch 251/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1833\n",
            "Epoch 252/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1853\n",
            "Epoch 253/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1878\n",
            "Epoch 254/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1856\n",
            "Epoch 255/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1813\n",
            "Epoch 256/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1870\n",
            "Epoch 257/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1812\n",
            "Epoch 258/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1852\n",
            "Epoch 259/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1845\n",
            "Epoch 260/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1849\n",
            "Epoch 261/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1836\n",
            "Epoch 262/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1838\n",
            "Epoch 263/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1811\n",
            "Epoch 264/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1872\n",
            "Epoch 265/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1837\n",
            "Epoch 266/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1831\n",
            "Epoch 267/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1855\n",
            "Epoch 268/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1890\n",
            "Epoch 269/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1874\n",
            "Epoch 270/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1874\n",
            "Epoch 271/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1820\n",
            "Epoch 272/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1852\n",
            "Epoch 273/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1828\n",
            "Epoch 274/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1854\n",
            "Epoch 275/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1862\n",
            "Epoch 276/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1849\n",
            "Epoch 277/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1865\n",
            "Epoch 278/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1826\n",
            "Epoch 279/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1843\n",
            "Epoch 280/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1832\n",
            "Epoch 281/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1808\n",
            "Epoch 282/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1897\n",
            "Epoch 283/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1806\n",
            "Epoch 284/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1832\n",
            "Epoch 285/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1839\n",
            "Epoch 286/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1880\n",
            "Epoch 287/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1876\n",
            "Epoch 288/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1853\n",
            "Epoch 289/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1815\n",
            "Epoch 290/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1829\n",
            "Epoch 291/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1820\n",
            "Epoch 292/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1841\n",
            "Epoch 293/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1792\n",
            "Epoch 294/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1826\n",
            "Epoch 295/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1817\n",
            "Epoch 296/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1830\n",
            "Epoch 297/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1849\n",
            "Epoch 298/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1836\n",
            "Epoch 299/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1847\n",
            "Epoch 300/300\n",
            "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1806\n",
            "✅ Stage1 預訓練完成，模型與embedding模型已儲存。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stage 2 聯合模型訓練與儲存"
      ],
      "metadata": {
        "id": "xX6YO0RPPm76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 必要套件 =====\n",
        "!pip install optuna tensorflow -q\n",
        "\n",
        "import os\n",
        "import joblib\n",
        "import optuna\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "\n",
        "# ===== 固定 SEED，確保重現 =====\n",
        "GLOBAL_SEED = 42\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "tf.random.set_seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "\n",
        "# ===== 參數設定 =====\n",
        "MODEL_DIR = '/content/drive/MyDrive/rul_diffusion_fd001'\n",
        "SEQ_LEN, D, T = 32, 16, 1000\n",
        "\n",
        "# ===== 載入資料 =====\n",
        "X_all = np.load(f'{MODEL_DIR}/X_all.npy')         # shape: (N, 32, 11)\n",
        "y_all = np.load(f'{MODEL_DIR}/y_all.npy')         # shape: (N,)\n",
        "unit_all = np.load(f'{MODEL_DIR}/unit_all.npy')   # shape: (N,)\n",
        "\n",
        "# ===== 分組：80 engines 訓練 / 20 engines 驗證 =====\n",
        "train_units = set(range(1, 81))\n",
        "val_units = set(range(81, 101))\n",
        "\n",
        "X_train, y_train, X_val, y_val = [], [], [], []\n",
        "for x, y, u in zip(X_all, y_all, unit_all):\n",
        "    if u in train_units:\n",
        "        X_train.append(x)\n",
        "        y_train.append(y)\n",
        "    else:\n",
        "        X_val.append(x)\n",
        "        y_val.append(y)\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_val, y_val = np.array(X_val), np.array(y_val)\n",
        "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
        "\n",
        "# ===== Diffusion timestep embedding =====\n",
        "def get_timestep_embedding_np(timesteps, dim):\n",
        "    timesteps = np.array(timesteps).reshape(-1, 1)\n",
        "    half_dim = dim // 2\n",
        "    emb = np.log(10000) / (half_dim - 1)\n",
        "    emb = np.exp(np.arange(half_dim) * -emb)\n",
        "    emb = timesteps * emb\n",
        "    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n",
        "    return emb.astype(np.float32)\n",
        "\n",
        "t_train = np.random.randint(1, T + 1, size=len(X_train))\n",
        "tau_train = get_timestep_embedding_np(t_train, D)\n",
        "t_val = np.random.randint(1, T + 1, size=len(X_val))\n",
        "tau_val = get_timestep_embedding_np(t_val, D)\n",
        "\n",
        "# ===== 載入 Stage1 Encoder embedding 模型 =====\n",
        "encoder = tf.keras.models.load_model(f'{MODEL_DIR}/stage1_encoder_embedding.keras', compile=False)\n",
        "encoder.trainable = True  # 可微調\n",
        "\n",
        "# ===== RUL 預測 MLP 架構 =====\n",
        "def build_rul_predictor(input_dim, hidden_units, dropout, activation):\n",
        "    inputs = Input(shape=(input_dim,))\n",
        "    x = inputs\n",
        "    for h in hidden_units:\n",
        "        x = layers.Dense(h, activation=activation)(x)\n",
        "        x = layers.Dropout(dropout)(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    return Model(inputs, output)\n",
        "\n",
        "# ===== Optuna objective =====\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 5e-4, log=True)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.2)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
        "    hidden_units = [trial.suggest_int(f\"n_units_l{i}\", 32, 128, step=32) for i in range(num_layers)]\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
        "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"gelu\", \"swish\"])\n",
        "    lambda_high = trial.suggest_float(\"lambda_high\", 0.01, 0.1)\n",
        "    patience_es = trial.suggest_int(\"patience_es\", 10, 20)\n",
        "    patience_rlr = trial.suggest_int(\"patience_rlr\", 3, 7)\n",
        "    def custom_rul_loss(lambda_high=lambda_high):\n",
        "        def loss(y_true, y_pred):\n",
        "            base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n",
        "            high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n",
        "            return base + lambda_high * high_bias\n",
        "        return loss\n",
        "\n",
        "    input_x = Input(shape=(SEQ_LEN, X_train.shape[-1]))\n",
        "    input_t = Input(shape=(D,))\n",
        "    encoded = encoder([input_x, input_t])\n",
        "    mlp = build_rul_predictor(encoded.shape[-1], hidden_units, dropout, activation)\n",
        "    output = mlp(encoded)\n",
        "\n",
        "    joint_model = Model([input_x, input_t], output)\n",
        "    joint_model.compile(optimizer=AdamW(learning_rate=learning_rate), loss=custom_rul_loss())\n",
        "\n",
        "    joint_model.fit([X_train, tau_train], y_train,\n",
        "                    validation_data=([X_val, tau_val], y_val),\n",
        "                    epochs=200, batch_size=batch_size, verbose=0,\n",
        "                    callbacks=[\n",
        "                        tf.keras.callbacks.EarlyStopping(patience=patience_es, restore_best_weights=True),\n",
        "                        tf.keras.callbacks.ReduceLROnPlateau(patience=patience_rlr, factor=0.5, min_lr=1e-7),\n",
        "                        TFKerasPruningCallback(trial, \"val_loss\")\n",
        "                    ])\n",
        "    y_pred = joint_model.predict([X_val, tau_val], batch_size=batch_size).flatten()\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    return rmse\n",
        "\n",
        "# ===== 超參數搜尋 =====\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED))\n",
        "study.optimize(objective, n_trials=100, timeout=1800)\n",
        "\n",
        "# ===== 重訓最佳模型 =====\n",
        "best = study.best_params\n",
        "best_hidden = [best[f\"n_units_l{i}\"] for i in range(2)]\n",
        "\n",
        "input_x = Input(shape=(SEQ_LEN, X_train.shape[-1]))\n",
        "input_t = Input(shape=(D,))\n",
        "encoded = encoder([input_x, input_t])\n",
        "mlp = build_rul_predictor(encoded.shape[-1], best_hidden, best['dropout'], best['activation'])\n",
        "output = mlp(encoded)\n",
        "\n",
        "def custom_rul_loss(lambda_high=best['lambda_high']):\n",
        "    def loss(y_true, y_pred):\n",
        "        base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n",
        "        high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n",
        "        return base + lambda_high * high_bias\n",
        "    return loss\n",
        "\n",
        "final_model = Model([input_x, input_t], output)\n",
        "final_model.compile(optimizer=AdamW(learning_rate=best['learning_rate']), loss=custom_rul_loss())\n",
        "\n",
        "final_model.fit([X_train, tau_train], y_train,\n",
        "                validation_data=([X_val, tau_val], y_val),\n",
        "                epochs=80,\n",
        "                batch_size=best['batch_size'], verbose=2,\n",
        "                callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(patience=best['patience_es'], restore_best_weights=True),\n",
        "                    tf.keras.callbacks.ReduceLROnPlateau(patience=best['patience_rlr'], factor=0.5, min_lr=1e-7)\n",
        "                ])\n",
        "\n",
        "final_model.save(f'{MODEL_DIR}/stage2_joint_model_best.h5')\n",
        "\n",
        "# ===== 驗證集結果評估 =====\n",
        "y_pred = final_model.predict([X_val, tau_val], batch_size=best['batch_size']).flatten()\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "print(f\"✅ 最佳參數: {best}\")\n",
        "print(f\"✅ 驗證集最小 RMSE (piecewise RUL): {rmse:.4f}\")\n",
        "print(f\"📁 模型已儲存至: {MODEL_DIR}/stage2_joint_model_best.h5\")\n",
        "\n",
        "# ===== 儲存 Optuna 搜尋結果 =====\n",
        "df_trials = study.trials_dataframe()\n",
        "df_trials.to_csv(f\"{MODEL_DIR}/optuna_search_results.csv\", index=False)\n",
        "print(f\"✅ Optuna 搜尋結果已儲存至: {MODEL_DIR}/optuna_search_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE3JLYnR_67l",
        "outputId": "3bf81c2b-ba0a-49c6-cb58-ffcfe282145a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:27:14,969] A new study created in memory with name: no-name-2c8b7132-e8aa-40b7-90c1-d08aec2363bf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (13658, 32, 11) y_train: (13658,)\n",
            "X_val: (3873, 32, 11) y_val: (3873,)\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:28:48,052] Trial 0 finished with value: 20.094353697919956 and parameters: {'learning_rate': 0.00018272261776066238, 'dropout': 0.19507143064099164, 'num_layers': 3, 'n_units_l0': 96, 'n_units_l1': 32, 'n_units_l2': 32, 'batch_size': 32, 'activation': 'gelu', 'lambda_high': 0.029110519961044856, 'patience_es': 12, 'patience_rlr': 3}. Best is trial 0 with value: 20.094353697919956.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:29:33,865] Trial 1 finished with value: 16.847217436465954 and parameters: {'learning_rate': 0.00016317596956423415, 'dropout': 0.1524756431632238, 'num_layers': 2, 'n_units_l0': 64, 'n_units_l1': 96, 'batch_size': 128, 'activation': 'relu', 'lambda_high': 0.06331731119758383, 'patience_es': 10, 'patience_rlr': 6}. Best is trial 1 with value: 16.847217436465954.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:31:34,287] Trial 2 finished with value: 16.079671861327544 and parameters: {'learning_rate': 0.00013158041368516616, 'dropout': 0.10650515929852795, 'num_layers': 4, 'n_units_l0': 128, 'n_units_l1': 128, 'n_units_l2': 64, 'n_units_l3': 32, 'batch_size': 16, 'activation': 'gelu', 'lambda_high': 0.06962700559185839, 'patience_es': 13, 'patience_rlr': 5}. Best is trial 2 with value: 16.079671861327544.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:33:44,589] Trial 3 finished with value: 14.75894669576252 and parameters: {'learning_rate': 0.00024106495902171608, 'dropout': 0.1184854455525527, 'num_layers': 4, 'n_units_l0': 128, 'n_units_l1': 128, 'n_units_l2': 128, 'n_units_l3': 96, 'batch_size': 16, 'activation': 'gelu', 'lambda_high': 0.08458637582367365, 'patience_es': 13, 'patience_rlr': 4}. Best is trial 3 with value: 14.75894669576252.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:34:48,658] Trial 4 finished with value: 16.99496654717076 and parameters: {'learning_rate': 0.00023951255499127385, 'dropout': 0.11409242249747627, 'num_layers': 4, 'n_units_l0': 32, 'n_units_l1': 128, 'n_units_l2': 128, 'n_units_l3': 32, 'batch_size': 32, 'activation': 'relu', 'lambda_high': 0.020428215357261678, 'patience_es': 19, 'patience_rlr': 6}. Best is trial 3 with value: 14.75894669576252.\n",
            "[I 2025-06-01 06:34:59,045] Trial 5 pruned. Trial was pruned at epoch 1.\n",
            "[I 2025-06-01 06:35:06,999] Trial 6 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:35:27,720] Trial 7 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:35:35,519] Trial 8 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:35:41,369] Trial 9 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:35:51,599] Trial 10 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:36:02,964] Trial 11 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:36:13,185] Trial 12 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:36:27,676] Trial 13 pruned. Trial was pruned at epoch 1.\n",
            "[I 2025-06-01 06:36:40,623] Trial 14 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:36:50,810] Trial 15 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:37:00,940] Trial 16 pruned. Trial was pruned at epoch 0.\n",
            "[I 2025-06-01 06:37:14,245] Trial 17 pruned. Trial was pruned at epoch 1.\n",
            "[I 2025-06-01 06:37:25,743] Trial 18 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:39:48,697] Trial 19 finished with value: 14.804698837396714 and parameters: {'learning_rate': 0.0002672559785280302, 'dropout': 0.1729613420412326, 'num_layers': 3, 'n_units_l0': 128, 'n_units_l1': 96, 'n_units_l2': 96, 'batch_size': 16, 'activation': 'swish', 'lambda_high': 0.04230003426339334, 'patience_es': 16, 'patience_rlr': 6}. Best is trial 3 with value: 14.75894669576252.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:40:25,696] Trial 20 finished with value: 16.350606905904325 and parameters: {'learning_rate': 0.00027316699089507084, 'dropout': 0.17850246049068946, 'num_layers': 3, 'n_units_l0': 96, 'n_units_l1': 96, 'n_units_l2': 128, 'batch_size': 64, 'activation': 'swish', 'lambda_high': 0.040328590012438346, 'patience_es': 17, 'patience_rlr': 6}. Best is trial 3 with value: 14.75894669576252.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:42:21,634] Trial 21 finished with value: 15.09674257422902 and parameters: {'learning_rate': 0.00022573296745517627, 'dropout': 0.18453462396036838, 'num_layers': 4, 'n_units_l0': 128, 'n_units_l1': 128, 'n_units_l2': 96, 'n_units_l3': 128, 'batch_size': 16, 'activation': 'swish', 'lambda_high': 0.04223617488198843, 'patience_es': 14, 'patience_rlr': 6}. Best is trial 3 with value: 14.75894669576252.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:44:15,783] Trial 22 finished with value: 15.89490132663911 and parameters: {'learning_rate': 0.0002307681314165468, 'dropout': 0.1822146526825931, 'num_layers': 3, 'n_units_l0': 128, 'n_units_l1': 96, 'n_units_l2': 96, 'batch_size': 16, 'activation': 'swish', 'lambda_high': 0.04074549171854379, 'patience_es': 15, 'patience_rlr': 7}. Best is trial 3 with value: 14.75894669576252.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:45:56,827] Trial 23 finished with value: 15.868528473770073 and parameters: {'learning_rate': 0.00033554281199923075, 'dropout': 0.16654475480996112, 'num_layers': 4, 'n_units_l0': 128, 'n_units_l1': 128, 'n_units_l2': 96, 'n_units_l3': 128, 'batch_size': 16, 'activation': 'swish', 'lambda_high': 0.03581030592961036, 'patience_es': 16, 'patience_rlr': 6}. Best is trial 3 with value: 14.75894669576252.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 06:47:25,692] Trial 24 finished with value: 17.28928314081985 and parameters: {'learning_rate': 0.00026864837944213653, 'dropout': 0.18124410035834423, 'num_layers': 2, 'n_units_l0': 128, 'n_units_l1': 96, 'batch_size': 16, 'activation': 'swish', 'lambda_high': 0.01502652938440592, 'patience_es': 14, 'patience_rlr': 6}. Best is trial 3 with value: 14.75894669576252.\n",
            "[I 2025-06-01 06:48:40,462] Trial 25 pruned. Trial was pruned at epoch 27.\n",
            "[I 2025-06-01 06:49:54,948] Trial 26 pruned. Trial was pruned at epoch 26.\n",
            "[I 2025-06-01 06:50:11,340] Trial 27 pruned. Trial was pruned at epoch 2.\n",
            "[I 2025-06-01 06:50:33,941] Trial 28 pruned. Trial was pruned at epoch 20.\n",
            "[I 2025-06-01 06:51:14,537] Trial 29 pruned. Trial was pruned at epoch 25.\n",
            "[I 2025-06-01 06:51:49,442] Trial 30 pruned. Trial was pruned at epoch 10.\n",
            "[I 2025-06-01 06:52:30,513] Trial 31 pruned. Trial was pruned at epoch 12.\n",
            "[I 2025-06-01 06:52:46,984] Trial 32 pruned. Trial was pruned at epoch 2.\n",
            "[I 2025-06-01 06:53:03,232] Trial 33 pruned. Trial was pruned at epoch 2.\n",
            "[I 2025-06-01 06:53:58,953] Trial 34 pruned. Trial was pruned at epoch 18.\n",
            "[I 2025-06-01 06:54:20,959] Trial 35 pruned. Trial was pruned at epoch 18.\n",
            "[I 2025-06-01 06:54:46,302] Trial 36 pruned. Trial was pruned at epoch 10.\n",
            "[I 2025-06-01 06:55:11,069] Trial 37 pruned. Trial was pruned at epoch 6.\n",
            "[I 2025-06-01 06:55:47,128] Trial 38 pruned. Trial was pruned at epoch 10.\n",
            "[I 2025-06-01 06:55:54,397] Trial 39 pruned. Trial was pruned at epoch 1.\n",
            "[I 2025-06-01 06:56:07,893] Trial 40 pruned. Trial was pruned at epoch 1.\n",
            "[I 2025-06-01 06:56:39,661] Trial 41 pruned. Trial was pruned at epoch 9.\n",
            "[I 2025-06-01 06:57:02,568] Trial 42 pruned. Trial was pruned at epoch 6.\n",
            "[I 2025-06-01 06:57:34,931] Trial 43 pruned. Trial was pruned at epoch 18.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "854/854 - 8s - 10ms/step - loss: 734.2474 - val_loss: 493.2842 - learning_rate: 2.4106e-04\n",
            "Epoch 2/80\n",
            "854/854 - 2s - 3ms/step - loss: 242.6398 - val_loss: 391.5890 - learning_rate: 2.4106e-04\n",
            "Epoch 3/80\n",
            "854/854 - 2s - 3ms/step - loss: 201.5226 - val_loss: 367.9125 - learning_rate: 2.4106e-04\n",
            "Epoch 4/80\n",
            "854/854 - 2s - 3ms/step - loss: 182.8866 - val_loss: 373.9537 - learning_rate: 2.4106e-04\n",
            "Epoch 5/80\n",
            "854/854 - 2s - 3ms/step - loss: 174.0871 - val_loss: 361.5573 - learning_rate: 2.4106e-04\n",
            "Epoch 6/80\n",
            "854/854 - 2s - 3ms/step - loss: 167.2253 - val_loss: 396.4471 - learning_rate: 2.4106e-04\n",
            "Epoch 7/80\n",
            "854/854 - 2s - 3ms/step - loss: 161.0314 - val_loss: 322.3735 - learning_rate: 2.4106e-04\n",
            "Epoch 8/80\n",
            "854/854 - 2s - 3ms/step - loss: 156.8647 - val_loss: 384.6866 - learning_rate: 2.4106e-04\n",
            "Epoch 9/80\n",
            "854/854 - 2s - 3ms/step - loss: 150.5642 - val_loss: 345.8916 - learning_rate: 2.4106e-04\n",
            "Epoch 10/80\n",
            "854/854 - 2s - 3ms/step - loss: 145.7627 - val_loss: 383.7415 - learning_rate: 2.4106e-04\n",
            "Epoch 11/80\n",
            "854/854 - 2s - 3ms/step - loss: 142.8590 - val_loss: 359.2949 - learning_rate: 2.4106e-04\n",
            "Epoch 12/80\n",
            "854/854 - 2s - 3ms/step - loss: 133.0032 - val_loss: 300.1754 - learning_rate: 1.2053e-04\n",
            "Epoch 13/80\n",
            "854/854 - 2s - 3ms/step - loss: 130.6717 - val_loss: 313.4138 - learning_rate: 1.2053e-04\n",
            "Epoch 14/80\n",
            "854/854 - 2s - 3ms/step - loss: 131.3209 - val_loss: 287.0658 - learning_rate: 1.2053e-04\n",
            "Epoch 15/80\n",
            "854/854 - 2s - 3ms/step - loss: 128.9303 - val_loss: 285.5004 - learning_rate: 1.2053e-04\n",
            "Epoch 16/80\n",
            "854/854 - 2s - 3ms/step - loss: 125.7355 - val_loss: 305.9520 - learning_rate: 1.2053e-04\n",
            "Epoch 17/80\n",
            "854/854 - 2s - 3ms/step - loss: 125.5856 - val_loss: 286.5690 - learning_rate: 1.2053e-04\n",
            "Epoch 18/80\n",
            "854/854 - 2s - 3ms/step - loss: 121.8221 - val_loss: 301.1532 - learning_rate: 1.2053e-04\n",
            "Epoch 19/80\n",
            "854/854 - 2s - 3ms/step - loss: 122.0284 - val_loss: 291.2247 - learning_rate: 1.2053e-04\n",
            "Epoch 20/80\n",
            "854/854 - 2s - 3ms/step - loss: 115.8147 - val_loss: 287.5076 - learning_rate: 6.0266e-05\n",
            "Epoch 21/80\n",
            "854/854 - 2s - 3ms/step - loss: 116.0265 - val_loss: 296.0172 - learning_rate: 6.0266e-05\n",
            "Epoch 22/80\n",
            "854/854 - 2s - 3ms/step - loss: 114.4955 - val_loss: 314.1596 - learning_rate: 6.0266e-05\n",
            "Epoch 23/80\n",
            "854/854 - 2s - 3ms/step - loss: 114.1774 - val_loss: 285.3363 - learning_rate: 6.0266e-05\n",
            "Epoch 24/80\n",
            "854/854 - 2s - 3ms/step - loss: 113.9974 - val_loss: 297.8732 - learning_rate: 6.0266e-05\n",
            "Epoch 25/80\n",
            "854/854 - 2s - 3ms/step - loss: 112.6231 - val_loss: 297.6206 - learning_rate: 6.0266e-05\n",
            "Epoch 26/80\n",
            "854/854 - 2s - 3ms/step - loss: 113.3838 - val_loss: 285.9136 - learning_rate: 6.0266e-05\n",
            "Epoch 27/80\n",
            "854/854 - 2s - 3ms/step - loss: 111.2449 - val_loss: 298.7735 - learning_rate: 6.0266e-05\n",
            "Epoch 28/80\n",
            "854/854 - 2s - 3ms/step - loss: 109.6913 - val_loss: 301.2676 - learning_rate: 3.0133e-05\n",
            "Epoch 29/80\n",
            "854/854 - 2s - 3ms/step - loss: 107.4386 - val_loss: 309.6025 - learning_rate: 3.0133e-05\n",
            "Epoch 30/80\n",
            "854/854 - 2s - 3ms/step - loss: 107.3677 - val_loss: 303.4621 - learning_rate: 3.0133e-05\n",
            "Epoch 31/80\n",
            "854/854 - 2s - 3ms/step - loss: 106.9731 - val_loss: 325.2153 - learning_rate: 3.0133e-05\n",
            "Epoch 32/80\n",
            "854/854 - 2s - 3ms/step - loss: 106.3442 - val_loss: 298.8844 - learning_rate: 1.5067e-05\n",
            "Epoch 33/80\n",
            "854/854 - 2s - 3ms/step - loss: 105.5490 - val_loss: 303.7560 - learning_rate: 1.5067e-05\n",
            "Epoch 34/80\n",
            "854/854 - 2s - 3ms/step - loss: 106.0498 - val_loss: 300.0945 - learning_rate: 1.5067e-05\n",
            "Epoch 35/80\n",
            "854/854 - 2s - 3ms/step - loss: 106.7137 - val_loss: 305.8075 - learning_rate: 1.5067e-05\n",
            "Epoch 36/80\n",
            "854/854 - 2s - 3ms/step - loss: 105.1952 - val_loss: 312.7801 - learning_rate: 7.5333e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "✅ 最佳參數: {'learning_rate': 0.00024106495902171608, 'dropout': 0.1184854455525527, 'num_layers': 4, 'n_units_l0': 128, 'n_units_l1': 128, 'n_units_l2': 128, 'n_units_l3': 96, 'batch_size': 16, 'activation': 'gelu', 'lambda_high': 0.08458637582367365, 'patience_es': 13, 'patience_rlr': 4}\n",
            "✅ 驗證集最小 RMSE (piecewise RUL): 17.1066\n",
            "📁 模型已儲存至: /content/drive/MyDrive/rul_diffusion_fd001/stage2_joint_model_best.h5\n",
            "✅ Optuna 搜尋結果已儲存至: /content/drive/MyDrive/rul_diffusion_fd001/optuna_search_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test資料集預處理"
      ],
      "metadata": {
        "id": "NeVUiEUQ4i97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "MODEL_DIR = '/content/drive/MyDrive/rul_diffusion_fd001'\n",
        "\n",
        "# === 1. 固定 SEQ_LEN ===\n",
        "SEQ_LEN = 32\n",
        "print(\"使用固定 SEQ_LEN:\", SEQ_LEN)\n",
        "\n",
        "# === 2. 載入 scaler、特徵名（這裡是11個特徵）===\n",
        "scaler = joblib.load(f'{MODEL_DIR}/scaler_preprocessed.joblib')\n",
        "feature_names = joblib.load(f'{MODEL_DIR}/feature_names.pkl')  # 11個特徵名列表\n",
        "\n",
        "# === 3. 載入 Test 原始資料 ===\n",
        "columns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\n",
        "test = pd.read_csv('/content/cmapss_data/CMaps/test_FD001.txt', sep='\\s+', header=None)\n",
        "test.columns = columns\n",
        "rul_truth = pd.read_csv('/content/cmapss_data/CMaps/RUL_FD001.txt', header=None, names=['RUL'])\n",
        "\n",
        "# === 4. 單一 alpha=0.2 指數平滑 ===\n",
        "def exponential_smoothing(series, alpha=0.2):\n",
        "    result = [series.iloc[0]]\n",
        "    for n in range(1, len(series)):\n",
        "        result.append(alpha * series.iloc[n] + (1 - alpha) * result[-1])\n",
        "    return pd.Series(result, index=series.index)\n",
        "\n",
        "for col in feature_names:\n",
        "    test[col] = test.groupby('unit')[col].transform(lambda x: exponential_smoothing(x, alpha=0.2))\n",
        "\n",
        "# === 5. 特徵正規化 ===\n",
        "test[feature_names] = scaler.transform(test[feature_names])\n",
        "\n",
        "# === 6. 產生 Test 視窗 ===\n",
        "X_test_list, timestep_list, units_list = [], [], []\n",
        "for unit in sorted(test['unit'].unique()):\n",
        "    df_unit = test[test['unit'] == unit]\n",
        "    arr = df_unit[feature_names].values\n",
        "    if len(arr) >= SEQ_LEN:\n",
        "        X_test_list.append(arr[-SEQ_LEN:])\n",
        "        timestep_list.append(df_unit['time'].values[-1])\n",
        "        units_list.append(unit)\n",
        "X_test = np.stack(X_test_list).astype(np.float32)\n",
        "\n",
        "# === 7. Timestep embedding ===\n",
        "def get_timestep_embedding_np(timesteps, dim):\n",
        "    timesteps = np.array(timesteps).reshape(-1, 1)\n",
        "    half_dim = dim // 2\n",
        "    emb = np.log(10000) / (half_dim - 1)\n",
        "    emb = np.exp(np.arange(half_dim) * -emb)\n",
        "    emb = timesteps * emb\n",
        "    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n",
        "    return emb.astype(np.float32)\n",
        "\n",
        "D = 16\n",
        "tau_test = get_timestep_embedding_np(timestep_list, D)\n",
        "\n",
        "# === 8. 載入真實RUL（依unit排序） ===\n",
        "y_test = rul_truth['RUL'].values  # 確保跟units_list順序一致，若不一致需對齊\n",
        "\n",
        "# === 9. 儲存資料 ===\n",
        "np.save(f'{MODEL_DIR}/X_test.npy', X_test)\n",
        "np.save(f'{MODEL_DIR}/tau_test.npy', tau_test)\n",
        "np.save(f'{MODEL_DIR}/units_list.npy', np.array(units_list))\n",
        "np.save(f'{MODEL_DIR}/y_test.npy', y_test)\n",
        "\n",
        "print(f\"✅ Test 視窗數: {len(X_test)}\")\n",
        "print(f\"🧪 對齊 unit 數量: {len(np.unique(units_list))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKkrGfptDHf_",
        "outputId": "f07c9386-9c35-424b-fb51-cd2f7b8406aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用固定 SEQ_LEN: 32\n",
            "✅ Test 視窗數: 99\n",
            "🧪 對齊 unit 數量: 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test驗證、計算RMSE與NASA Score、繪圖並存檔"
      ],
      "metadata": {
        "id": "gQZgU__zVw9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.manifold import TSNE\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "MODEL_DIR = '/content/drive/MyDrive/rul_diffusion_fd001'\n",
        "\n",
        "# === 1. 載入 TestSet 前處理資料 ===\n",
        "X_test = np.load(f'{MODEL_DIR}/X_test.npy')\n",
        "tau_test = np.load(f'{MODEL_DIR}/tau_test.npy')\n",
        "units_list = np.load(f'{MODEL_DIR}/units_list.npy').astype(int)  # 必須是 int 型態\n",
        "\n",
        "# === 1-2. 載入 NASA ground-truth RUL 並與 units_list 對齊 ===\n",
        "rul_truth = pd.read_csv('/content/cmapss_data/CMaps/RUL_FD001.txt', header=None, names=['RUL'])\n",
        "y_true = rul_truth.loc[units_list - 1, 'RUL'].values\n",
        "\n",
        "# === 2. 自動抓訓練時最佳 lambda_high ===\n",
        "optuna_result_csv = f'{MODEL_DIR}/optuna_search_results.csv'\n",
        "optuna_df = pd.read_csv(optuna_result_csv)\n",
        "if 'params_lambda_high' in optuna_df.columns:\n",
        "    best_trial = optuna_df.loc[optuna_df['value'].idxmin()]\n",
        "    best_lambda_high = best_trial['params_lambda_high']\n",
        "else:\n",
        "    best_lambda_high = 0.1  # 預設值\n",
        "\n",
        "def custom_rul_loss(lambda_high=best_lambda_high):\n",
        "    def loss(y_true, y_pred):\n",
        "        base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n",
        "        high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n",
        "        return base + lambda_high * high_bias\n",
        "    return loss\n",
        "\n",
        "# === 3. 載入訓練好的 Stage2 聯合模型 ===\n",
        "joint_model = load_model(\n",
        "    f'{MODEL_DIR}/stage2_joint_model_best.h5',\n",
        "    custom_objects={'loss': custom_rul_loss()}\n",
        ")\n",
        "\n",
        "# === 4. 預測 ===\n",
        "y_pred = joint_model.predict([X_test, tau_test], verbose=1).flatten()\n",
        "y_pred = np.clip(y_pred, 0, 130)\n",
        "\n",
        "# === 5. NASA Score 函數 ===\n",
        "def nasa_score(y_true, y_pred):\n",
        "    score = 0\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        d = pred - true\n",
        "        if d < 0:\n",
        "            score += np.exp(-d / 13) - 1\n",
        "        else:\n",
        "            score += np.exp(d / 10) - 1\n",
        "    return score\n",
        "\n",
        "# === 6. 計算分數 ===\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "score = nasa_score(y_true, y_pred)\n",
        "print(f'✅ Test RMSE: {rmse:.4f}')\n",
        "print(f'✅ Test NASA Score: {score:.4f}')\n",
        "\n",
        "# === 7. 儲存預測結果 ===\n",
        "df_out = pd.DataFrame({\n",
        "    'unit': units_list,\n",
        "    'True_RUL': y_true,\n",
        "    'Pred_RUL': y_pred,\n",
        "    'Error': y_pred - y_true\n",
        "})\n",
        "df_out.to_csv(f'{MODEL_DIR}/test_pred_result.csv', index=False)\n",
        "print(f\"📁 預測結果已儲存：{MODEL_DIR}/test_pred_result.csv\")\n",
        "\n",
        "# === 8. 圖表: 預測 vs 真實 RUL ===\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(y_true, label='True RUL', marker='o', alpha=0.8)\n",
        "plt.plot(y_pred, label='Predicted RUL', marker='x', alpha=0.8)\n",
        "plt.xlabel('Test Unit (Engine)')\n",
        "plt.ylabel('RUL')\n",
        "plt.legend()\n",
        "plt.title(f'Predicted vs. True RUL (Test)\\nRMSE={rmse:.2f} | NASA Score={score:.2f}')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{MODEL_DIR}/test_pred_vs_true_rul.png')\n",
        "plt.close()\n",
        "\n",
        "# === 9. 殘差分布圖 (Error vs True RUL) ===\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(y_true, y_pred - y_true, alpha=0.75, c='royalblue', edgecolor='k')\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('True RUL')\n",
        "plt.ylabel('Prediction Error (Predicted - True)')\n",
        "plt.title('Residual Distribution (Error vs True RUL)')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{MODEL_DIR}/test_residual_scatter.png')\n",
        "plt.close()\n",
        "\n",
        "# === 10. 預測誤差直方圖 ===\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.hist(y_pred - y_true, bins=25, color='skyblue', edgecolor='black')\n",
        "plt.title('Prediction Error Histogram')\n",
        "plt.xlabel('Prediction Error (Predicted - True)')\n",
        "plt.ylabel('Count')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{MODEL_DIR}/test_error_histogram.png')\n",
        "plt.close()\n",
        "\n",
        "# === 11. t-SNE 可視化 Stage1 Encoder Embedding 特徵（可選） ===\n",
        "try:\n",
        "    encoder = load_model(f'{MODEL_DIR}/stage1_encoder_embedding.keras', compile=False)\n",
        "    features = encoder.predict([X_test, tau_test])\n",
        "    features_2d = TSNE(n_components=2, random_state=42).fit_transform(features)\n",
        "    plt.figure(figsize=(7,6))\n",
        "    sc = plt.scatter(features_2d[:,0], features_2d[:,1], c=y_true, cmap='viridis', s=20)\n",
        "    plt.colorbar(sc, label='True RUL')\n",
        "    plt.title(\"t-SNE of Encoder Embedding Features (Test set)\")\n",
        "    plt.xlabel(\"t-SNE Dim 1\")\n",
        "    plt.ylabel(\"t-SNE Dim 2\")\n",
        "    plt.savefig(f'{MODEL_DIR}/test_encoder_tsne.png')\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"[t-SNE 可視化失敗] {e}\")\n",
        "\n",
        "print(f\"📈 圖片已儲存：\\n{MODEL_DIR}/test_pred_vs_true_rul.png\\n{MODEL_DIR}/test_residual_scatter.png\\n{MODEL_DIR}/test_error_histogram.png\\n{MODEL_DIR}/test_encoder_tsne.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8enHtcUbDx1_",
        "outputId": "9067f9e2-b7c4-49f3-af19-9a7261a1c4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229ms/step\n",
            "✅ Test RMSE: 16.4323\n",
            "✅ Test NASA Score: 449.7096\n",
            "📁 預測結果已儲存：/content/drive/MyDrive/rul_diffusion_fd001/test_pred_result.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 248 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b334f5a76a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 179ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 251 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b334f5a76a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "📈 圖片已儲存：\n",
            "/content/drive/MyDrive/rul_diffusion_fd001/test_pred_vs_true_rul.png\n",
            "/content/drive/MyDrive/rul_diffusion_fd001/test_residual_scatter.png\n",
            "/content/drive/MyDrive/rul_diffusion_fd001/test_error_histogram.png\n",
            "/content/drive/MyDrive/rul_diffusion_fd001/test_encoder_tsne.png\n"
          ]
        }
      ]
    }
  ]
}