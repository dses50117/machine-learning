{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1579356,"sourceType":"datasetVersion","datasetId":933960}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TRAIN資料集預處理（FD003）","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport shap\n\n# === 1. 全局 SEED（確保 deterministic）===\nGLOBAL_SEED = 42\nrandom.seed(GLOBAL_SEED)\nnp.random.seed(GLOBAL_SEED)\n\n# === 2. 基本參數與路徑 ===\nMODEL_DIR = '/kaggle/working'\nos.makedirs(MODEL_DIR, exist_ok=True)\nmax_rul = 130\nwindow_size = 12\nthreshold = 0.2\n\n# FD003: 26個感測器 + 5欄 (unit, time, op1, op2, op3)\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 27)]\n\ndef load_and_label(filepath, max_rul=130):\n    df = pd.read_csv(filepath, sep='\\s+', header=None, dtype=float, engine='python')\n    print(f\"原始 shape: {df.shape}\")\n    if df.shape[1] < len(columns):\n        for i in range(len(columns) - df.shape[1]):\n            df[f'extra_{i}'] = 0.0\n    if df.shape[1] > len(columns):\n        df = df.iloc[:, :len(columns)]\n    df.columns = columns\n    df['RUL_linear'] = df.groupby('unit')['time'].transform('max') - df['time']\n    df['RUL_linear'] = df['RUL_linear'].clip(upper=max_rul)\n    return df\n\n# FD003 訓練集路徑\ntrain = load_and_label('/kaggle/input/cmapssdata/train_FD003.txt', max_rul=max_rul)\nprint(\"Train loaded. Shape:\", train.shape)\n\ndef generate_piecewise_rul_algorithm1(df, feature_cols, window_size=12, threshold=0.2, max_rul=130, verbose=False):\n    result_dfs = []\n    for unit_id, group in df.groupby('unit'):\n        group = group.sort_values('time').reset_index(drop=True)\n        sensor_data = group[feature_cols].values\n        num_cycles = len(group)\n        num_windows = num_cycles // window_size\n\n        if num_windows < 3:\n            irul = num_cycles\n            group['iRUL'] = irul\n            group['RUL_piecewise'] = np.clip(irul - group['time'], 0, max_rul)\n            result_dfs.append(group)\n            continue\n\n        centroids = [np.mean(sensor_data[i * window_size:(i + 1) * window_size], axis=0)\n                     for i in range(num_windows)]\n        base = centroids[0]\n        degradation_found = False\n\n        for i in range(2, num_windows):\n            dist_sq = np.sum((centroids[i] - base) ** 2)\n            if verbose:\n                print(f\"[Unit {unit_id}] Compare w1 to w{i+1}: Dist^2 = {dist_sq:.4f}\")\n            if dist_sq >= threshold:\n                degradation_start = i * window_size\n                irul = num_cycles - degradation_start\n                degradation_found = True\n                break\n\n        if not degradation_found:\n            irul = num_cycles\n\n        rul_piecewise = [\n            irul if t <= (num_cycles - irul) else irul - (t - (num_cycles - irul))\n            for t in range(num_cycles)\n        ]\n        group['iRUL'] = irul\n        group['RUL_piecewise'] = np.clip(rul_piecewise, 0, max_rul)\n        result_dfs.append(group)\n\n    return pd.concat(result_dfs, ignore_index=True)\n\n# 5. 皮爾森高於10%才保留\nsensor_cols = [f's{i}' for i in range(1, 27)]\nsensor_cols_keep = [col for col in sensor_cols if train[col].std() >= 1e-3]\n\n# 6. Z-score 去離群\ndef z_score_filter(df, cols, threshold=4.0):\n    for col in cols:\n        mean = df[col].mean()\n        std = df[col].std()\n        z = (df[col] - mean) / std\n        df.loc[z.abs() > threshold, col] = mean\n    return df\n\ntrain = z_score_filter(train, sensor_cols_keep)\n\n# 7. 分段線性RUL標註\ntrain = generate_piecewise_rul_algorithm1(\n    train, sensor_cols_keep, window_size=window_size, threshold=threshold, max_rul=max_rul, verbose=False\n)\n\n# 8. 特徵選擇 (皮爾森+SHAP) \nprint(\"開始訓練 RandomForest（部分資料做SHAP，防止OOM）\")\nsample_n = min(2000, len(train))\ntrain_sample = train.sample(n=sample_n, random_state=GLOBAL_SEED)\nrf = RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1)\nrf.fit(train_sample[sensor_cols_keep], train_sample['RUL_piecewise'])\n\nshap_sample_n = min(200, len(train_sample))\nshap_sample_idx = np.random.choice(len(train_sample), shap_sample_n, replace=False)\nshap_X = train_sample[sensor_cols_keep].iloc[shap_sample_idx]\nexplainer = shap.Explainer(rf, shap_X)\nshap_values = explainer(shap_X, check_additivity=False)\nmean_shap = np.abs(shap_values.values).mean(axis=0)\nshap_scores = pd.Series(mean_shap, index=sensor_cols_keep).sort_values(ascending=False)\ntop8_shap = shap_scores.head(8).index.tolist()\n\ncorrs = train[sensor_cols_keep + ['RUL_piecewise']].corr()['RUL_piecewise'].abs().sort_values(ascending=False)\ntop8_pearson = corrs.drop('RUL_piecewise').head(8).index.tolist()\n\nselected_sensors = [x for x in top8_shap if x in top8_pearson]\nfor x in top8_pearson:\n    if x not in selected_sensors:\n        selected_sensors.append(x)\n    if len(selected_sensors) == 8:\n        break\n\nprint(\"最終選用的8個感測器:\", selected_sensors)\n\n# 9. 構建最終特徵（含OP/差分）\nfeatures = selected_sensors + ['op1', 'op2', 'op3']\n\ndef multi_exponential_smoothing(series, alphas=[0.1, 0.3]):\n    results = []\n    for alpha in alphas:\n        smoothed = [series.iloc[0]]\n        for n in range(1, len(series)):\n            smoothed.append(alpha * series.iloc[n] + (1 - alpha) * smoothed[-1])\n        results.append(pd.Series(smoothed, index=series.index))\n    return sum(results) / len(results)\n\nfor col in features:\n    train[col] = train.groupby('unit')[col].transform(lambda x: multi_exponential_smoothing(x)).astype('float64')\n\nfor col in selected_sensors:\n    train[f'{col}_diff'] = train.groupby('unit')[col].diff().fillna(0)\n\nfinal_features = features + [f'{col}_diff' for col in selected_sensors]\n\n# 10. 標準化\nscaler = StandardScaler()\ntrain[final_features] = scaler.fit_transform(train[final_features])\n\n# 11. 儲存所有重要資訊\njoblib.dump(scaler, f'{MODEL_DIR}/scaler_preprocessed.joblib')\njoblib.dump(final_features, f'{MODEL_DIR}/feature_names.pkl')\ntrain.to_csv(f'{MODEL_DIR}/train_with_piecewise_rul.csv', index=False)\n\nprint('🚩 已完成分段線性RUL資料與所有特徵處理，檔案儲存：')\nprint(f'- {MODEL_DIR}/train_with_piecewise_rul.csv')\nprint(f'- {MODEL_DIR}/feature_names.pkl')\nprint(f'- {MODEL_DIR}/scaler_preprocessed.joblib')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:41:51.042759Z","iopub.execute_input":"2025-05-31T14:41:51.043358Z","iopub.status.idle":"2025-05-31T14:42:14.095851Z","shell.execute_reply.started":"2025-05-31T14:41:51.043305Z","shell.execute_reply":"2025-05-31T14:42:14.095177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optuna訓練（FD003）","metadata":{}},{"cell_type":"code","source":"!pip install optuna optuna-integration[tfkeras] -q\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport optuna\nfrom optuna_integration import TFKerasPruningCallback\n\nDATASET = 'fd003'\nVALIDATION_SPLIT = 0.2\nGLOBAL_SEED = 42\n\nMODEL_DIR = '/kaggle/working'\nDATA_PATH = f'{MODEL_DIR}/train_with_piecewise_rul.csv'\nSCALER_PATH = f'{MODEL_DIR}/scaler_preprocessed.joblib'\nFEATURE_PATH = f'{MODEL_DIR}/feature_names.pkl'\nMODEL_PATH = f'{MODEL_DIR}/best_{DATASET}_lstm_model_mse.keras'\nRESULT_CSV = f'{MODEL_DIR}/{DATASET}_optuna_search_results.csv'\nVAL_UNITS_PATH = f'{MODEL_DIR}/{DATASET}_val_units.npy'\nTRAIN_UNITS_PATH = f'{MODEL_DIR}/{DATASET}_train_units.npy'\n\nnp.random.seed(GLOBAL_SEED)\ntf.random.set_seed(GLOBAL_SEED)\n\nfeature_cols = joblib.load(FEATURE_PATH)\nscaler = joblib.load(SCALER_PATH)\n\ndf_raw = pd.read_csv(DATA_PATH)\nrul_col = \"RUL_piecewise\"\nmin_irul = df_raw['iRUL'].min()\nnp.save(f'{MODEL_DIR}/{DATASET}_min_irul.npy', min_irul)\nprint(f\"📉 使用動態 CLIP RUL（最小 iRUL）: {min_irul}\")\n\nall_units = df_raw['unit'].unique()\nrng = np.random.default_rng(GLOBAL_SEED)\n\nif os.path.exists(VAL_UNITS_PATH) and os.path.exists(TRAIN_UNITS_PATH):\n    val_units = np.load(VAL_UNITS_PATH)\n    train_units = np.load(TRAIN_UNITS_PATH)\n    print(\"✅ 已載入現有的 validation/train 分割名單\")\nelse:\n    val_units = rng.choice(all_units, int(len(all_units) * VALIDATION_SPLIT), replace=False)\n    train_units = [u for u in all_units if u not in val_units]\n    np.save(VAL_UNITS_PATH, val_units)\n    np.save(TRAIN_UNITS_PATH, train_units)\n    print(\"✅ 首次建立 validation/train 分割並儲存\")\n\ntrain_df = df_raw[df_raw['unit'].isin(train_units)]\nval_df = df_raw[df_raw['unit'].isin(val_units)]\n\ndef make_lstm_dataset(df, feature_cols, window_size, stride=1, target_col='RUL_piecewise', clip_value=None):\n    sequences, labels = [], []\n    for _, group in df.groupby('unit'):\n        data = group.sort_values('time')\n        X = data[feature_cols].values\n        y = data[target_col].values\n        if clip_value is not None:\n            y = np.clip(y, 0, clip_value)\n        for i in range(0, len(data) - window_size + 1, stride):\n            sequences.append(X[i:i+window_size])\n            labels.append(y[i+window_size-1])\n    return np.array(sequences), np.array(labels)\n\ndef phm08_score(y_true, y_pred):\n    error = y_pred - y_true\n    score = np.where(error < 0,\n                     np.exp(-error / 13) - 1,\n                     np.exp(error / 10) - 1)\n    return np.sum(score)\n\ndef build_model(input_shape, lstm_units, dropout_rate, learning_rate, optimizer_name, num_layers):\n    model = Sequential()\n    model.add(tf.keras.layers.Input(shape=input_shape))\n    for i in range(num_layers):\n        return_seq = (i < num_layers - 1)\n        model.add(LSTM(lstm_units, return_sequences=return_seq))\n        model.add(Dropout(dropout_rate))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n    optimizers = {\n        'adam': tf.keras.optimizers.Adam(learning_rate),\n        'sgd': tf.keras.optimizers.SGD(learning_rate),\n        'rmsprop': tf.keras.optimizers.RMSprop(learning_rate),\n        'nadam': tf.keras.optimizers.Nadam(learning_rate)\n    }\n    model.compile(optimizer=optimizers[optimizer_name],\n                  loss='mse',\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model\n\ndef objective(trial):\n    seed = trial.suggest_categorical('seed', [11, 22, 33])\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    lstm_units = trial.suggest_categorical('lstm_units', [192, 256, 320])\n    dropout_rate = trial.suggest_float('dropout_rate', 0.13, 0.22)\n    window_size = trial.suggest_categorical('window_size', [64, 80, 96, 100, 112, 120, 128])\n    stride = trial.suggest_categorical('stride', [1])\n    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n    learning_rate = trial.suggest_float('learning_rate', 0.0002, 0.0015, log=True)\n    optimizer = trial.suggest_categorical('optimizer', ['nadam'])\n    num_layers = trial.suggest_int('num_layers', 1, 2)\n    epochs = trial.suggest_int('epochs', 120, 200)\n\n    X_train, y_train = make_lstm_dataset(train_df, feature_cols, window_size, stride, target_col=rul_col, clip_value=min_irul)\n    X_val, y_val = make_lstm_dataset(val_df, feature_cols, window_size, stride, target_col=rul_col, clip_value=min_irul)\n\n    model = build_model(\n        input_shape=(window_size, X_train.shape[2]),\n        lstm_units=lstm_units,\n        dropout_rate=dropout_rate,\n        learning_rate=learning_rate,\n        optimizer_name=optimizer,\n        num_layers=num_layers\n    )\n    early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=0)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=0)\n    pruning_callback = TFKerasPruningCallback(trial, 'val_loss')\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=epochs,\n        batch_size=batch_size,\n        callbacks=[early_stop, reduce_lr, pruning_callback],\n        verbose=0\n    )\n\n    val_rmse = model.evaluate(X_val, y_val, verbose=0)[1]\n    y_pred = model.predict(X_val, verbose=0).flatten()\n    val_score = phm08_score(y_val, y_pred)\n\n    trial.set_user_attr(\"VAL_RMSE\", val_rmse)\n    trial.set_user_attr(\"VAL_SCORE\", val_score)\n\n    return val_rmse\n\nstudy = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED))\nstudy.optimize(objective, n_trials=80, timeout=21600)\n\nprint(\"✅ Optuna 搜尋完畢！\")\nprint(\"Best params:\", study.best_trial.params)\nprint(\"Best RMSE:\", study.best_value)\n\noptuna_df = study.trials_dataframe()\noptuna_df.to_csv(RESULT_CSV, index=False)\n\nbest_params = study.best_trial.params\nnp.random.seed(best_params['seed'])\ntf.random.set_seed(best_params['seed'])\n\nX_train, y_train = make_lstm_dataset(train_df, feature_cols, best_params['window_size'], best_params['stride'], target_col=rul_col, clip_value=min_irul)\nX_val, y_val = make_lstm_dataset(val_df, feature_cols, best_params['window_size'], best_params['stride'], target_col=rul_col, clip_value=min_irul)\n\nbest_model = build_model(\n    input_shape=(best_params['window_size'], X_train.shape[2]),\n    lstm_units=best_params['lstm_units'],\n    dropout_rate=best_params['dropout_rate'],\n    learning_rate=best_params['learning_rate'],\n    optimizer_name=best_params['optimizer'],\n    num_layers=best_params['num_layers']\n)\nbest_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=best_params['epochs'],\n    batch_size=best_params['batch_size'],\n    callbacks=[\n        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n    ],\n    verbose=1\n)\nbest_model.save(MODEL_PATH)\nprint(f\"📁 最佳模型儲存於：{MODEL_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:42:14.096940Z","iopub.execute_input":"2025-05-31T14:42:14.097171Z","iopub.status.idle":"2025-05-31T14:46:23.979960Z","shell.execute_reply.started":"2025-05-31T14:42:14.097154Z","shell.execute_reply":"2025-05-31T14:46:23.979378Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 測試集資料處理","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\nimport os\n\nMODEL_DIR = '/kaggle/working'\n\noptuna_df = pd.read_csv(f'{MODEL_DIR}/fd003_optuna_search_results.csv')\nsort_col = 'user_attrs_VAL_RMSE' if 'user_attrs_VAL_RMSE' in optuna_df.columns else 'value'\nbest_trial = optuna_df.loc[optuna_df[sort_col].idxmin()]\nSEQ_LEN = int(best_trial['params_window_size'])\nprint(f\"✅ 最佳 SEQ_LEN = {SEQ_LEN}\")\n\nfeature_names = joblib.load(f'{MODEL_DIR}/feature_names.pkl')\nscaler = joblib.load(f'{MODEL_DIR}/scaler_preprocessed.joblib')\n\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 27)]\ntest = pd.read_csv('/kaggle/input/cmapssdata/test_FD003.txt', sep='\\s+', header=None)\nif test.shape[1] < len(columns):\n    for i in range(len(columns) - test.shape[1]):\n        test[f'extra_{i}'] = 0.0\nif test.shape[1] > len(columns):\n    test = test.iloc[:, :len(columns)]\ntest.columns = columns\n\nrul_truth = pd.read_csv('/kaggle/input/cmapssdata/RUL_FD003.txt', header=None, names=['true_RUL'])\n\ndef multi_exponential_smoothing(series, alphas=[0.1, 0.3]):\n    results = []\n    for alpha in alphas:\n        smoothed = [series.iloc[0]]\n        for n in range(1, len(series)):\n            smoothed.append(alpha * series.iloc[n] + (1 - alpha) * smoothed[-1])\n        results.append(pd.Series(smoothed, index=series.index))\n    return sum(results) / len(results)\n\nfor col in [c for c in feature_names if not c.endswith('_diff') and c not in ['unit', 'time']]:\n    test[col] = test.groupby('unit')[col].transform(lambda x: multi_exponential_smoothing(x))\n\nfor col in [c for c in feature_names if c.endswith('_diff')]:\n    base_col = col.replace('_diff', '')\n    test[col] = test.groupby('unit')[base_col].diff().fillna(0)\n\ntest[feature_names] = scaler.transform(test[feature_names])\n\nX_test, unit_ids = [], []\nfor unit in sorted(test['unit'].unique()):\n    df_unit = test[test['unit'] == unit]\n    arr = df_unit[feature_names].values\n    if len(arr) >= SEQ_LEN:\n        X_test.append(arr[-SEQ_LEN:])\n        unit_ids.append(unit)\nX_test = np.stack(X_test).astype(np.float32)\ny_true = rul_truth.loc[np.array(unit_ids) - 1, 'true_RUL'].values\n\nnp.save(f'{MODEL_DIR}/X_test.npy', X_test)\nnp.save(f'{MODEL_DIR}/unit_ids.npy', np.array(unit_ids))\nnp.save(f'{MODEL_DIR}/y_true.npy', y_true)\n\nprint(f\"✅ 測試集預處理完成 X_test.shape={X_test.shape}，unit數量={len(unit_ids)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:46:23.980825Z","iopub.execute_input":"2025-05-31T14:46:23.981056Z","iopub.status.idle":"2025-05-31T14:46:26.129353Z","shell.execute_reply.started":"2025-05-31T14:46:23.981035Z","shell.execute_reply":"2025-05-31T14:46:26.128601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FD003測試集驗證","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nimport random\nimport os\n\nMODEL_DIR = '/kaggle/working'\nGLOBAL_SEED = 42\nrandom.seed(GLOBAL_SEED)\nnp.random.seed(GLOBAL_SEED)\ntf.random.set_seed(GLOBAL_SEED)\n\ndef phm08_score(y_true, y_pred):\n    error = y_pred - y_true\n    score = np.where(error < 0,\n                     np.exp(-error / 13) - 1,\n                     np.exp(error / 10) - 1)\n    return np.sum(score)\n\ndef multi_exponential_smoothing(series, alphas=[0.1, 0.3]):\n    results = []\n    for alpha in alphas:\n        smoothed = [series.iloc[0]]\n        for n in range(1, len(series)):\n            smoothed.append(alpha * series.iloc[n] + (1 - alpha) * smoothed[-1])\n        results.append(pd.Series(smoothed, index=series.index))\n    return sum(results) / len(results)\n\ndef preprocess_for_engine(df, feature_names, scaler):\n    for col in [c for c in feature_names if not c.endswith('_diff') and c not in ['unit','time']]:\n        df[col] = multi_exponential_smoothing(df[col])\n    for col in [c for c in feature_names if c.endswith('_diff')]:\n        base_col = col.replace('_diff', '')\n        df[col] = df[base_col].diff().fillna(0)\n    df[feature_names] = scaler.transform(df[feature_names])\n    return df\n\ndef safe_assign_columns(df, columns):\n    if df.shape[1] < len(columns):\n        for i in range(len(columns) - df.shape[1]):\n            df[f'extra_{i}'] = 0.0\n    if df.shape[1] > len(columns):\n        df = df.iloc[:, :len(columns)]\n    df.columns = columns\n    return df\n\nX_test = np.load(f'{MODEL_DIR}/X_test.npy')\nunit_ids = np.load(f'{MODEL_DIR}/unit_ids.npy')\ny_true = np.load(f'{MODEL_DIR}/y_true.npy')\n\nMODEL_PATH = f'{MODEL_DIR}/best_fd003_lstm_model_mse.keras'\nfeature_cols = joblib.load(f'{MODEL_DIR}/feature_names.pkl')\nscaler = joblib.load(f'{MODEL_DIR}/scaler_preprocessed.joblib')\nmodel = tf.keras.models.load_model(MODEL_PATH)\n\noptuna_df = pd.read_csv(f'{MODEL_DIR}/fd003_optuna_search_results.csv')\nsort_col = 'user_attrs_VAL_RMSE' if 'user_attrs_VAL_RMSE' in optuna_df.columns else 'value'\nbest_trial = optuna_df.loc[optuna_df[sort_col].idxmin()]\nSEQ_LEN = int(best_trial['params_window_size'])\n\ntry:\n    clip_upper = int(np.load(f'{MODEL_DIR}/fd003_min_irul.npy'))\n    print(f\"✅ 自動 clip 上限：{clip_upper}\")\nexcept Exception as e:\n    clip_upper = 130\n    print(f\"⚠️ 找不到 min_irul，預設 clip 上限：{clip_upper} ({e})\")\n\ny_pred = model.predict(X_test, verbose=0).flatten()\ny_pred = np.clip(y_pred, 0, clip_upper)\n\nrmse = np.sqrt(mean_squared_error(y_true, y_pred))\nscore = phm08_score(y_true, y_pred)\n\ndf_out = pd.DataFrame({\n    'engine_id': unit_ids,\n    'true_RUL': y_true,\n    'predicted_RUL': y_pred,\n    'error': y_pred - y_true\n})\ndf_out['score_component'] = np.where(df_out['error'] < 0,\n                                     np.exp(-df_out['error'] / 13) - 1,\n                                     np.exp(df_out['error'] / 10) - 1)\ndf_out['RMSE'] = rmse\ndf_out['Score'] = score\ndf_out.to_csv(f'{MODEL_DIR}/fd003_test_rul_results.csv', index=False)\n\nplt.figure(figsize=(12, 6))\nplt.plot(df_out['engine_id'], df_out['true_RUL'], label='True RUL', marker='o')\nplt.plot(df_out['engine_id'], df_out['predicted_RUL'], label='Predicted RUL', marker='x')\nplt.title(f'FD003: True vs Predicted RUL\\nRMSE={rmse:.2f} | Score={score:.2f}')\nplt.xlabel('Engine ID')\nplt.ylabel('RUL')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd003_test_rul_plot.png')\nplt.close()\n\nplt.figure(figsize=(10, 5))\nplt.hist(df_out['error'], bins=25, color='skyblue', edgecolor='black')\nplt.title('FD003 Prediction Error Histogram')\nplt.xlabel('Prediction Error (Predicted - True)')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd003_error_histogram.png')\nplt.close()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(df_out['true_RUL'], df_out['error'], c='royalblue', alpha=0.7, edgecolors='k')\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('True RUL')\nplt.ylabel('Prediction Error (Predicted - True)')\nplt.title('Residual Distribution (Error vs True RUL)')\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd003_residual_scatter.png')\nplt.close()\n\nSAMPLE_ENGINE_ID = int(df_out.iloc[df_out['error'].abs().argmax()]['engine_id'])\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 27)]\ntest_raw = pd.read_csv('/kaggle/input/cmapssdata/test_FD003.txt', sep='\\s+', header=None)\ntest_raw = safe_assign_columns(test_raw, columns)\nsample_df = test_raw[test_raw['unit'] == SAMPLE_ENGINE_ID].sort_values('time').reset_index(drop=True)\nsample_df = preprocess_for_engine(sample_df, feature_cols, scaler)\nsample_features = sample_df[feature_cols].values\n\nsample_preds = []\nfor i in range(SEQ_LEN, len(sample_df) + 1):\n    seq_x = sample_features[i-SEQ_LEN:i]\n    pred = model.predict(seq_x[np.newaxis, :, :], verbose=0).flatten()[0]\n    sample_preds.append(pred)\nsample_ruls = np.arange(len(sample_df)-1, -1, -1)[:len(sample_preds)]\n\nplt.figure(figsize=(12, 6))\nplt.plot(range(len(sample_preds)), sample_preds, label='Predicted RUL', marker='x')\nplt.plot(range(len(sample_ruls)), sample_ruls, label='True RUL', marker='o')\nplt.title(f'Engine {SAMPLE_ENGINE_ID}: RUL Prediction (Full Cycle)')\nplt.xlabel('Cycle')\nplt.ylabel('RUL')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd003_sample_engine_curve.png')\nplt.close()\n\nprint(f\"\\n✅ 測試集 RMSE = {rmse:.4f}\")\nprint(f\"✅ 測試集 Score = {score:.4f}\")\nprint(f\"📊 最大誤差：{df_out['error'].max():.2f}\")\nprint(f\"📊 最小誤差：{df_out['error'].min():.2f}\")\nprint(f\"📊 超過 ±100 預測數量：{np.sum(np.abs(df_out['error']) > 100)}\")\nprint(f\"📈 圖片已儲存：\")\nprint(f\"{MODEL_DIR}/fd003_test_rul_plot.png\")\nprint(f\"{MODEL_DIR}/fd003_error_histogram.png\")\nprint(f\"{MODEL_DIR}/fd003_residual_scatter.png\")\nprint(f\"{MODEL_DIR}/fd003_sample_engine_curve.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:46:26.131031Z","iopub.execute_input":"2025-05-31T14:46:26.131464Z","iopub.status.idle":"2025-05-31T14:46:31.142032Z","shell.execute_reply.started":"2025-05-31T14:46:26.131445Z","shell.execute_reply":"2025-05-31T14:46:31.141395Z"}},"outputs":[],"execution_count":null}]}