{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1579356,"sourceType":"datasetVersion","datasetId":933960}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TRAINË≥áÊñôÂâçËôïÁêÜ","metadata":{}},{"cell_type":"code","source":"!pip install -q optuna optuna-integration[tfkeras]\nimport os, random, numpy as np, pandas as pd, joblib\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport shap\nfrom tqdm import tqdm\n\nGLOBAL_SEED = 42\nrandom.seed(GLOBAL_SEED)\nnp.random.seed(GLOBAL_SEED)\nMODEL_DIR = '/kaggle/working'\nSEQ_LEN = 32\nmax_rul = 130\nwindow_size = 12\nthreshold = 0.2\n\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\ntrain = pd.read_csv('/kaggle/input/cmapssdata/train_FD003.txt', sep='\\s+', header=None)\ntrain.columns = columns\n\ndef generate_piecewise_rul(df, feature_cols, window_size=12, threshold=0.2, max_rul=130):\n    result_dfs = []\n    for unit_id, group in df.groupby('unit'):\n        group = group.sort_values('time').reset_index(drop=True)\n        sensor_data = group[feature_cols].values\n        num_cycles = len(group)\n        num_windows = num_cycles // window_size\n        if num_windows < 3:\n            irul = num_cycles\n            group['iRUL'] = irul\n            group['RUL_piecewise'] = np.clip(irul - group['time'], 0, max_rul)\n            result_dfs.append(group)\n            continue\n        centroids = [np.mean(sensor_data[i*window_size:(i+1)*window_size], axis=0) for i in range(num_windows)]\n        base = centroids[0]\n        degradation_found = False\n        for i in range(2, num_windows):\n            dist_sq = np.sum((centroids[i] - base) ** 2)\n            if dist_sq >= threshold:\n                degradation_start = i * window_size\n                irul = num_cycles - degradation_start\n                degradation_found = True\n                break\n        if not degradation_found:\n            irul = num_cycles\n        rul_piecewise = [irul if t <= (num_cycles - irul) else irul - (t - (num_cycles - irul)) for t in range(num_cycles)]\n        group['iRUL'] = irul\n        group['RUL_piecewise'] = np.clip(rul_piecewise, 0, max_rul)\n        result_dfs.append(group)\n    return pd.concat(result_dfs, ignore_index=True)\n\nsensor_cols = [f's{i}' for i in range(1, 22)]\nsensor_cols_keep = [col for col in sensor_cols if train[col].std() >= 1e-3]\n\ndef z_score_filter(df, cols, threshold=4.0):\n    for col in cols:\n        mean = df[col].mean()\n        std = df[col].std()\n        z = (df[col] - mean) / std\n        df.loc[z.abs() > threshold, col] = mean\n    return df\n\ntrain = z_score_filter(train, sensor_cols_keep)\ntrain = generate_piecewise_rul(train, sensor_cols_keep, window_size, threshold, max_rul)\n\nsample_n = min(2000, len(train))\ntrain_sample = train.sample(n=sample_n, random_state=GLOBAL_SEED)\nrf = RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1)\nrf.fit(train_sample[sensor_cols_keep], train_sample['RUL_piecewise'])\n\nshap_sample_n = min(200, len(train_sample))\nshap_sample_idx = np.random.choice(len(train_sample), shap_sample_n, replace=False)\nshap_X = train_sample[sensor_cols_keep].iloc[shap_sample_idx]\nexplainer = shap.Explainer(rf, shap_X)\nshap_values = explainer(shap_X, check_additivity=False)\nmean_shap = np.abs(shap_values.values).mean(axis=0)\nshap_scores = pd.Series(mean_shap, index=sensor_cols_keep).sort_values(ascending=False)\ntop8_shap = shap_scores.head(8).index.tolist()\ncorrs = train[sensor_cols_keep + ['RUL_piecewise']].corr()['RUL_piecewise'].abs().sort_values(ascending=False)\ntop8_pearson = corrs.drop('RUL_piecewise').head(8).index.tolist()\nselected_sensors = [x for x in top8_shap if x in top8_pearson]\nfor x in top8_pearson:\n    if x not in selected_sensors:\n        selected_sensors.append(x)\n    if len(selected_sensors) == 8:\n        break\n\nfeatures = selected_sensors + ['op1', 'op2', 'op3']\n\ndef multi_exponential_smoothing(series, alphas=[0.1, 0.3]):\n    results = []\n    for alpha in alphas:\n        smoothed = [series.iloc[0]]\n        for n in range(1, len(series)):\n            smoothed.append(alpha * series.iloc[n] + (1 - alpha) * smoothed[-1])\n        results.append(pd.Series(smoothed, index=series.index))\n    return sum(results) / len(results)\n\nfor col in features:\n    train[col] = train.groupby('unit')[col].transform(lambda x: multi_exponential_smoothing(x)).astype('float64')\nfor col in selected_sensors:\n    train[f'{col}_diff'] = train.groupby('unit')[col].diff().fillna(0)\nfinal_features = features + [f'{col}_diff' for col in selected_sensors]\n\nscaler = StandardScaler()\ntrain[final_features] = scaler.fit_transform(train[final_features])\njoblib.dump(scaler, f'{MODEL_DIR}/fd003_scaler_preprocessed_Diff.joblib')\njoblib.dump(final_features, f'{MODEL_DIR}/fd003_feature_names_Diff.pkl')\ntrain.to_csv(f'{MODEL_DIR}/fd003_train_with_piecewise_rul_Diff.csv', index=False)\n\nX_list, y_list, unit_list = [], [], []\nfor unit in tqdm(train['unit'].unique(), desc='Áî¢ÁîüÊªëÂãïË¶ñÁ™ó'):\n    df_unit = train[train['unit'] == unit]\n    arr = df_unit[final_features].values\n    rul_piecewise = df_unit['RUL_piecewise'].values\n    if len(arr) < SEQ_LEN:\n        continue\n    for i in range(len(arr) - SEQ_LEN + 1):\n        X_list.append(arr[i:i + SEQ_LEN])\n        y_list.append(rul_piecewise[i + SEQ_LEN - 1])\n        unit_list.append(unit)\nX_all = np.stack(X_list).astype(np.float32)\ny_all = np.array(y_list).astype(np.float32)\nunit_all = np.array(unit_list).astype(int)\nnp.save(f'{MODEL_DIR}/fd003_X_all_Diff.npy', X_all)\nnp.save(f'{MODEL_DIR}/fd003_y_all_Diff.npy', y_all)\nnp.save(f'{MODEL_DIR}/fd003_unit_all_Diff.npy', unit_all)\nprint(f\"ÊªëÂãïË¶ñÁ™óÂÆåÊàê: X_all={X_all.shape}, y_all={y_all.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage1 Encoder È†êË®ìÁ∑¥","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, Input\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\nSEQ_LEN = 32\nD = 16\nT = 1000\nMODEL_DIR = '/kaggle/working'\n\nX_all = np.load(f'{MODEL_DIR}/fd003_X_all_Diff.npy')\n\ndef cbam_block(inputs, reduction_ratio=8):\n    channel = int(inputs.shape[-1])\n    avg_pool = layers.GlobalAveragePooling1D(keepdims=True)(inputs)\n    max_pool = layers.GlobalMaxPooling1D(keepdims=True)(inputs)\n    dense = layers.Dense(channel // reduction_ratio, activation='relu')\n    dense_out = layers.Dense(channel)\n    avg_out = dense_out(dense(avg_pool))\n    max_out = dense_out(dense(max_pool))\n    channel_attention = layers.Activation('sigmoid')(layers.Add()([avg_out, max_out]))\n    channel_refined = layers.Multiply()([inputs, channel_attention])\n    avg_pool_spatial = layers.GlobalAveragePooling1D(keepdims=True)(channel_refined)\n    max_pool_spatial = layers.GlobalMaxPooling1D(keepdims=True)(channel_refined)\n    concat = layers.Concatenate(axis=-1)([avg_pool_spatial, max_pool_spatial])\n    spatial_attention = layers.Conv1D(1, 7, padding='same', activation='sigmoid')(concat)\n    refined = layers.Multiply()([channel_refined, spatial_attention])\n    return refined\n\ndef resblock(x, filters, kernel_size=3, stride=1, add_noise=0.01, dropout=0.15):\n    shortcut = x\n    x = layers.Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.GaussianNoise(add_noise)(x)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = cbam_block(x)\n    if int(shortcut.shape[-1]) != filters:\n        shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n    x = layers.Add()([shortcut, x])\n    x = layers.Activation('relu')(x)\n    return x\n\ndef align_and_concat(x1, x2):\n    t1 = x1.shape[1]\n    t2 = x2.shape[1]\n    minlen = min(t1, t2)\n    if t1 > minlen:\n        x1 = layers.Cropping1D((0, t1 - minlen))(x1)\n    if t2 > minlen:\n        x2 = layers.Cropping1D((0, t2 - minlen))(x2)\n    return layers.Concatenate()([x1, x2])\n\ndef build_unet_encoder(input_dim, embed_dim, seq_len, base_filters=64, depth=4, drop_enc=0.18, noise_enc=0.06):\n    input_x = Input(shape=(seq_len, input_dim), name='input_1')\n    input_tau = Input(shape=(embed_dim,), name='input_2')\n    x = layers.GaussianNoise(noise_enc)(input_x)\n    t_proj = layers.Dense(input_dim)(input_tau)\n    t_proj_exp = layers.Reshape((1, input_dim))(t_proj)\n    x = layers.Add()([x, t_proj_exp])\n    skips = []\n    for d in range(depth):\n        filters = base_filters * (2 ** d)\n        x = resblock(x, filters, add_noise=noise_enc, dropout=drop_enc)\n        skips.append(x)\n        if d != depth - 1:\n            x = layers.MaxPooling1D(2)(x)\n    filters = base_filters * (2 ** depth)\n    x = resblock(x, filters, add_noise=noise_enc, dropout=drop_enc)\n    for d in reversed(range(depth)):\n        x = layers.UpSampling1D(2)(x)\n        x = align_and_concat(x, skips[d])\n        x = resblock(x, base_filters * (2 ** d), add_noise=noise_enc, dropout=drop_enc)\n    output = layers.Conv1D(input_dim, 1, padding='same')(x)\n    model = Model([input_x, input_tau], output)\n    return model\n\ndef cosine_beta_schedule(timesteps, s=0.008):\n    steps = timesteps + 1\n    x = np.linspace(0, timesteps, steps)\n    f = np.cos(((x / timesteps) + s) / (1 + s) * np.pi / 2) ** 2\n    alphas_cumprod = f / f[0]\n    betas = np.clip(1 - (alphas_cumprod[1:] / alphas_cumprod[:-1]), 0, 0.999)\n    return betas\n\nbetas = cosine_beta_schedule(T, s=0.008)\nalphas = 1 - betas\nalphas_cumprod = np.cumprod(alphas)\n\ndef get_timestep_embedding(timesteps, dim=D):\n    timesteps = tf.convert_to_tensor(timesteps, dtype=tf.float32)\n    timesteps = tf.reshape(timesteps, [-1, 1])\n    half_dim = dim // 2\n    emb = tf.math.log(10000.0) / (half_dim - 1)\n    emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n    emb = timesteps * emb\n    emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n    return emb\n\ndef diffusion_dataset(X, batch_size, T, alphas_cumprod, D):\n    def generator():\n        dataset_size = len(X)\n        while True:\n            idxs = np.random.permutation(dataset_size)\n            for i in range(0, dataset_size, batch_size):\n                batch_idx = idxs[i:i+batch_size]\n                x_start = X[batch_idx]\n                b = len(x_start)\n                t = np.random.randint(1, T + 1, size=b)\n                tau = get_timestep_embedding(t, D).numpy()\n                noise = np.random.randn(*x_start.shape).astype(np.float32)\n                sqrt_alpha = np.sqrt(alphas_cumprod[t - 1])[:, None, None]\n                sqrt_one_minus_alpha = np.sqrt(1 - alphas_cumprod[t - 1])[:, None, None]\n                x_t = sqrt_alpha * x_start + sqrt_one_minus_alpha * noise\n                yield {\"input_1\": x_t, \"input_2\": tau}, noise\n    output_signature = (\n        {\"input_1\": tf.TensorSpec(shape=(None, SEQ_LEN, X.shape[-1]), dtype=tf.float32),\n         \"input_2\": tf.TensorSpec(shape=(None, D), dtype=tf.float32)},\n        tf.TensorSpec(shape=(None, SEQ_LEN, X.shape[-1]), dtype=tf.float32)\n    )\n    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n\nBATCH_SIZE = 128\nEPOCHS_STAGE1 = 300\n\ntrain_ds = diffusion_dataset(X_all, BATCH_SIZE, T, alphas_cumprod, D)\nsteps_per_epoch = len(X_all) // BATCH_SIZE\n\nunet_encoder = build_unet_encoder(\n    input_dim=X_all.shape[-1], embed_dim=D, seq_len=SEQ_LEN,\n    base_filters=64, depth=4, drop_enc=0.18, noise_enc=0.06\n)\nunet_encoder.compile(optimizer=tf.keras.optimizers.Adam(5e-4), loss='mse')\n\nhistory = unet_encoder.fit(\n    train_ds,\n    epochs=EPOCHS_STAGE1,\n    steps_per_epoch=steps_per_epoch,\n    callbacks=[tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)]\n)\n\nprint(\"Stage1 Loss History:\")\nprint(history.history['loss'])\n\nplt.figure(figsize=(7,4))\nplt.plot(history.history['loss'], marker='o')\nplt.title(\"Stage1 Diffusion Encoder Loss Curve\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss (MSE)\")\nplt.grid()\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd003_stage1_encoder_loss_curve_Diff.png')\nplt.show()\n\nprint(f\"ÊúÄÂæåLossÂÄº: {history.history['loss'][-1]:.4f}\")\n\nweights = unet_encoder.get_weights()\nfor i, w in enumerate(weights):\n    print(f\"Layer {i} weight mean: {np.mean(w):.4f}, std: {np.std(w):.4f}\")\n\ngap = layers.GlobalAveragePooling1D()(unet_encoder.layers[-2].output)\nencoder_embedding_model = Model(unet_encoder.inputs, gap)\nunet_encoder.save(f'{MODEL_DIR}/fd003_stage1_encoder_full_Diff.keras')\nencoder_embedding_model.save(f'{MODEL_DIR}/fd003_stage1_encoder_embedding_Diff.keras')\nprint(\"‚úÖ FD003 Stage1 EncoderËàáEmbeddingÊ®°ÂûãÂ∑≤ÂÑ≤Â≠ò\")\nprint(f\"‚úÖ LossÊõ≤Á∑öÂÑ≤Â≠òÊñº {MODEL_DIR}/fd003_stage1_encoder_loss_curve_Diff.png\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage2 Optuna ÊêúÂ∞ã + RUL MLP","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport optuna\nfrom optuna.integration import TFKerasPruningCallback\nfrom tensorflow.keras import layers, Input, Model\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport numpy as np\nimport gc\n\nSEQ_LEN, D, T = 32, 16, 1000\nGLOBAL_SEED = 42\n\nMODEL_DIR = '/kaggle/working'\nmodel_path = '/kaggle/working/fd003_stage2_joint_model_best_Diff.keras'\noptuna_csv_path = '/kaggle/working/fd003_optuna_search_results_Diff.csv'\n\nX_all = np.load(f'{MODEL_DIR}/fd003_X_all_Diff.npy')\ny_all = np.load(f'{MODEL_DIR}/fd003_y_all_Diff.npy')\nunit_all = np.load(f'{MODEL_DIR}/fd003_unit_all_Diff.npy')\n\nunit_ids = np.unique(unit_all)\nn_unit = len(unit_ids)\nsplit = int(n_unit * 0.8)\ntrain_units = set(unit_ids[:split])\nval_units = set(unit_ids[split:])\n\nX_train, y_train, X_val, y_val = [], [], [], []\nfor x, y, u in zip(X_all, y_all, unit_all):\n    if u in train_units:\n        X_train.append(x)\n        y_train.append(y)\n    elif u in val_units:\n        X_val.append(x)\n        y_val.append(y)\nX_train = np.array(X_train, dtype=np.float32)\ny_train = np.array(y_train, dtype=np.float32)\nX_val = np.array(X_val, dtype=np.float32)\ny_val = np.array(y_val, dtype=np.float32)\n\ndef get_timestep_embedding_np(timesteps, dim):\n    timesteps = np.array(timesteps).reshape(-1, 1)\n    half_dim = dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim) * -emb)\n    emb = timesteps * emb\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    return emb.astype(np.float32)\n\nt_train = np.random.randint(1, T + 1, size=len(X_train))\ntau_train = get_timestep_embedding_np(t_train, D)\nt_val = np.full(len(X_val), SEQ_LEN)\ntau_val = get_timestep_embedding_np(t_val, D)\n\ndef mixup(x, t, y, alpha=0.2):\n    idx = np.random.permutation(len(x))\n    lam = np.random.beta(alpha, alpha)\n    x_mix = lam * x + (1 - lam) * x[idx]\n    t_mix = lam * t + (1 - lam) * t[idx]\n    y_mix = lam * y + (1 - lam) * y[idx]\n    return x_mix, t_mix, y_mix\n\ndef build_rul_predictor(input_dim, n_layers, hidden_units, dropout=0.18):\n    x_in = Input(shape=(input_dim,))\n    x = layers.BatchNormalization()(x_in)\n    for i in range(n_layers):\n        x = layers.Dense(hidden_units[i], activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        if i > 0:\n            x = layers.Dropout(dropout)(x)\n    y_out = layers.Dense(1, activation='linear')(x)\n    return Model(x_in, y_out)\n\ndef objective(trial):\n    tf.keras.backend.clear_session()\n    gc.collect()\n    encoder_embedding_model = tf.keras.models.load_model(\n        f'{MODEL_DIR}/fd003_stage1_encoder_embedding_Diff.keras', compile=False)\n    encoder_embedding_model.trainable = True\n\n    n_layers = trial.suggest_int(\"n_layers\", 2, 3)           # Âª∫Ë≠∞2~3Â±§\n    hidden_units = [\n        trial.suggest_int(\"h1\", 192, 320, step=32),          # Êõ¥ÂØ¨ÁØÑÂúç\n        trial.suggest_int(\"h2\", 96, 192, step=16),\n        trial.suggest_int(\"h3\", 48, 96, step=8),\n        trial.suggest_int(\"h4\", 16, 48, step=8),\n    ]\n    lr = trial.suggest_float(\"learning_rate\", 7e-5, 2e-4, log=True)\n    dropout = trial.suggest_float(\"dropout\", 0.15, 0.21)\n    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n    patience_es = trial.suggest_int(\"patience_es\", 12, 18)\n    patience_rlr = trial.suggest_int(\"patience_rlr\", 5, 9)\n    lambda_high = trial.suggest_float(\"lambda_high\", 0.06, 0.10)\n    mixup_alpha = trial.suggest_float(\"mixup_alpha\", 0.13, 0.20)\n\n    def custom_rul_loss(lambda_high=lambda_high):\n        def loss(y_true, y_pred):\n            base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n            high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n            return base + lambda_high * high_bias\n        return loss\n\n    input_x = Input(shape=(SEQ_LEN, X_train.shape[-1]), name=\"stage2_input_x\")\n    input_t = Input(shape=(D,), name=\"stage2_input_t\")\n    encoded = encoder_embedding_model([input_x, input_t])\n    mlp = build_rul_predictor(encoded.shape[-1], n_layers, hidden_units, dropout)\n    output = mlp(encoded)\n    joint_model = Model([input_x, input_t], output)\n    joint_model.compile(optimizer=AdamW(learning_rate=lr), loss=custom_rul_loss())\n\n    x_tr, t_tr, y_tr = X_train, tau_train, y_train\n    if mixup_alpha > 0:\n        x_tr, t_tr, y_tr = mixup(X_train, tau_train, y_train, alpha=mixup_alpha)\n\n    try:\n        history = joint_model.fit(\n            [x_tr, t_tr], y_tr,\n            validation_data=([X_val, tau_val], y_val),\n            epochs=50,\n            batch_size=batch_size,\n            verbose=1,\n            callbacks=[\n                tf.keras.callbacks.EarlyStopping(patience=patience_es, restore_best_weights=True),\n                tf.keras.callbacks.ReduceLROnPlateau(patience=patience_rlr, factor=0.5, min_lr=1e-7),\n                TFKerasPruningCallback(trial, \"val_loss\")\n            ]\n        )\n        y_pred = joint_model.predict([X_val, tau_val], batch_size=batch_size).flatten()\n        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    except Exception as e:\n        print(f\"[OOM or ERROR] {e}\")\n        rmse = 1e9\n\n    del joint_model, mlp, encoded, input_x, input_t, encoder_embedding_model\n    tf.keras.backend.clear_session()\n    gc.collect()\n    return rmse\n\nstudy = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED))\nstudy.optimize(objective, n_trials=80, timeout=7200)\n\nbest = study.best_params\nprint(f\"‚úÖ OptunaÊúÄ‰Ω≥ÂèÉÊï∏: {best}\")\n\ntf.keras.backend.clear_session()\ngc.collect()\nencoder_embedding_model = tf.keras.models.load_model(\n    f'{MODEL_DIR}/fd003_stage1_encoder_embedding_Diff.keras', compile=False)\nencoder_embedding_model.trainable = True\n\nbest_hidden_units = [\n    best[\"h1\"],\n    best[\"h2\"],\n    best[\"h3\"],\n    best[\"h4\"],\n]\ninput_x = Input(shape=(SEQ_LEN, X_train.shape[-1]), name=\"stage2_input_x_final\")\ninput_t = Input(shape=(D,), name=\"stage2_input_t_final\")\nencoded = encoder_embedding_model([input_x, input_t])\nmlp = build_rul_predictor(\n    encoded.shape[-1],\n    n_layers=best[\"n_layers\"],\n    hidden_units=best_hidden_units,\n    dropout=best[\"dropout\"],\n)\noutput = mlp(encoded)\n\ndef custom_rul_loss(lambda_high=best[\"lambda_high\"]):\n    def loss(y_true, y_pred):\n        base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n        high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n        return base + lambda_high * high_bias\n    return loss\n\nfinal_model = Model([input_x, input_t], output)\nfinal_model.compile(optimizer=AdamW(learning_rate=best['learning_rate']), loss=custom_rul_loss())\n\nif best.get('mixup_alpha', 0) > 0:\n    x_tr, t_tr, y_tr = mixup(X_train, tau_train, y_train, alpha=best['mixup_alpha'])\nelse:\n    x_tr, t_tr, y_tr = X_train, tau_train, y_train\n\nfinal_model.fit(\n    [x_tr, t_tr], y_tr,\n    validation_data=([X_val, tau_val], y_val),\n    epochs=100,\n    batch_size=best['batch_size'],\n    verbose=2,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=best['patience_es'], restore_best_weights=True),\n        tf.keras.callbacks.ReduceLROnPlateau(patience=best['patience_rlr'], factor=0.5, min_lr=1e-7)\n    ]\n)\n\nif os.path.exists(model_path):\n    if os.path.isfile(model_path):\n        os.remove(model_path)\n    else:\n        shutil.rmtree(model_path)\nfinal_model.save(model_path)\n\nstudy.trials_dataframe().to_csv(optuna_csv_path, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Ë≥áÊñôÂâçËôïÁêÜ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\n\nMODEL_DIR = '/kaggle/working'\nSEQ_LEN = 32\n\nscaler = joblib.load(f'{MODEL_DIR}/fd003_scaler_preprocessed_Diff.joblib')\nfeature_names = joblib.load(f'{MODEL_DIR}/fd003_feature_names_Diff.pkl')\n\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\ntest = pd.read_csv('/kaggle/input/cmapssdata/test_FD003.txt', sep='\\s+', header=None)\ntest.columns = columns\nrul_truth = pd.read_csv('/kaggle/input/cmapssdata/RUL_FD003.txt', header=None, names=['RUL'])\n\ndef exponential_smoothing(series, alpha=0.2):\n    result = [series.iloc[0]]\n    for n in range(1, len(series)):\n        result.append(alpha * series.iloc[n] + (1 - alpha) * result[-1])\n    return pd.Series(result, index=series.index)\n\nfor col in feature_names:\n    if col.endswith('_diff'):\n        continue\n    if col in test.columns:\n        test[col] = test.groupby('unit')[col].transform(lambda x: exponential_smoothing(x, alpha=0.2))\nfor col in feature_names:\n    if col.endswith('_diff'):\n        base_col = col.replace('_diff', '')\n        if base_col in test.columns:\n            test[col] = test.groupby('unit')[base_col].diff().fillna(0)\n        else:\n            test[col] = 0.0\nfor col in feature_names:\n    if col not in test.columns:\n        test[col] = 0.0\n\ntest[feature_names] = scaler.transform(test[feature_names])\n\nX_test_list, timestep_list, units_list = [], [], []\nfor unit in sorted(test['unit'].unique()):\n    df_unit = test[test['unit'] == unit]\n    arr = df_unit[feature_names].values\n    if len(arr) >= SEQ_LEN:\n        X_test_list.append(arr[-SEQ_LEN:])\n        timestep_list.append(df_unit['time'].values[-1])\n        units_list.append(unit)\nX_test = np.stack(X_test_list).astype(np.float32)\n\ndef get_timestep_embedding_np(timesteps, dim):\n    timesteps = np.array(timesteps).reshape(-1, 1)\n    half_dim = dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim) * -emb)\n    emb = timesteps * emb\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    return emb.astype(np.float32)\n\nD = 16\ntau_test = get_timestep_embedding_np(timestep_list, D)\ny_test = rul_truth['RUL'].values\n\nnp.save(f'{MODEL_DIR}/fd003_X_test_Diff.npy', X_test)\nnp.save(f'{MODEL_DIR}/fd003_tau_test_Diff.npy', tau_test)\nnp.save(f'{MODEL_DIR}/fd003_units_list_Diff.npy', np.array(units_list))\nnp.save(f'{MODEL_DIR}/fd003_y_test_Diff.npy', y_test)\n\nprint(f\"‚úÖ TestË¶ñÁ™óÊï∏: {len(X_test)}\")\nprint(f\"üß™ Â∞çÈΩäunitÊï∏Èáè: {len(np.unique(units_list))}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Ë©ï‰º∞ËàáË¶ñË¶∫Âåñ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.manifold import TSNE\nfrom tensorflow.keras.models import load_model\nimport tensorflow as tf\n\nMODEL_DIR = '/kaggle/working'\nX_test = np.load(f'{MODEL_DIR}/fd003_X_test_Diff.npy')\ntau_test = np.load(f'{MODEL_DIR}/fd003_tau_test_Diff.npy')\nunits_list = np.load(f'{MODEL_DIR}/fd003_units_list_Diff.npy').astype(int)\nrul_truth = pd.read_csv('/kaggle/input/cmapssdata/RUL_FD003.txt', header=None, names=['RUL'])\ny_true = rul_truth.loc[units_list - 1, 'RUL'].values\n\noptuna_result_csv = f'{MODEL_DIR}/fd003_optuna_search_results_Diff.csv'\noptuna_df = pd.read_csv(optuna_result_csv)\nif 'params_lambda_high' in optuna_df.columns:\n    best_lambda_high = optuna_df.loc[optuna_df['value'].idxmin()]['params_lambda_high']\nelse:\n    best_lambda_high = 0.1\n\ndef custom_rul_loss(lambda_high=best_lambda_high):\n    def loss(y_true, y_pred):\n        base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n        high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n        return base + lambda_high * high_bias\n    return loss\n\njoint_model = load_model(f'{MODEL_DIR}/fd003_stage2_joint_model_best_Diff.keras',\n                         custom_objects={'loss': custom_rul_loss()})\n\ny_pred = joint_model.predict([X_test, tau_test], verbose=1).flatten()\ny_pred = np.clip(y_pred, 0, 130)\n\ndef nasa_score(y_true, y_pred):\n    score = 0\n    for true, pred in zip(y_true, y_pred):\n        d = pred - true\n        if d < 0:\n            score += np.exp(-d / 13) - 1\n        else:\n            score += np.exp(d / 10) - 1\n    return score\n\nrmse = np.sqrt(mean_squared_error(y_true, y_pred))\nscore = nasa_score(y_true, y_pred)\nprint(f'‚úÖ Test RMSE: {rmse:.4f}')\nprint(f'‚úÖ Test NASA Score: {score:.4f}')\n\ndf_out = pd.DataFrame({\n    'unit': units_list,\n    'True_RUL': y_true,\n    'Pred_RUL': y_pred,\n    'Error': y_pred - y_true\n})\ndf_out.to_csv(f'{MODEL_DIR}/fd003_test_pred_result_Diff.csv', index=False)\nprint(f\"üìÅ È†êÊ∏¨ÁµêÊûúÂ∑≤ÂÑ≤Â≠òÔºö{MODEL_DIR}/fd003_test_pred_result_Diff.csv\")\n\nplt.figure(figsize=(8, 5))\nplt.plot(y_true, label='True RUL', marker='o', alpha=0.8)\nplt.plot(y_pred, label='Predicted RUL', marker='x', alpha=0.8)\nplt.xlabel('Test Unit (Engine)')\nplt.ylabel('RUL')\nplt.legend()\nplt.title(f'Predicted vs. True RUL (Test)\\nRMSE={rmse:.2f} | NASA Score={score:.2f}')\nplt.grid()\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd003_test_pred_vs_true_rul_Diff.png')\nplt.close()\n\nplt.figure(figsize=(7,5))\nplt.scatter(y_true, y_pred - y_true, alpha=0.75, c='royalblue', edgecolor='k')\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('True RUL')\nplt.ylabel('Prediction Error (Predicted - True)')\nplt.title('Residual Distribution (Error vs True RUL)')\nplt.grid()\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd003_test_residual_scatter_Diff.png')\nplt.close()\n\nplt.figure(figsize=(7,4))\nplt.hist(y_pred - y_true, bins=25, color='skyblue', edgecolor='black')\nplt.title('Prediction Error Histogram')\nplt.xlabel('Prediction Error (Predicted - True)')\nplt.ylabel('Count')\nplt.grid()\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/fd003_test_error_histogram_Diff.png')\nplt.close()\n\ntry:\n    encoder = load_model(f'{MODEL_DIR}/fd003_stage1_encoder_embedding_Diff.keras', compile=False)\n    features = encoder.predict([X_test, tau_test])\n    features_2d = TSNE(n_components=2, random_state=42).fit_transform(features)\n    plt.figure(figsize=(7,6))\n    sc = plt.scatter(features_2d[:,0], features_2d[:,1], c=y_true, cmap='viridis', s=20)\n    plt.colorbar(sc, label='True RUL')\n    plt.title(\"t-SNE of Encoder Embedding Features (Test set)\")\n    plt.xlabel(\"t-SNE Dim 1\")\n    plt.ylabel(\"t-SNE Dim 2\")\n    plt.savefig(f'{MODEL_DIR}/fd003_test_encoder_tsne_Diff.png')\n    plt.close()\nexcept Exception as e:\n    print(f\"[t-SNE ÂèØË¶ñÂåñÂ§±Êïó] {e}\")\n\nprint(f\"‚úÖ ÂúñÁâáÂ∑≤ÂÑ≤Â≠òÔºö\")\nprint(f\" - {MODEL_DIR}/fd003_test_pred_vs_true_rul_Diff.png\")\nprint(f\" - {MODEL_DIR}/fd003_test_residual_scatter_Diff.png\")\nprint(f\" - {MODEL_DIR}/fd003_test_error_histogram_Diff.png\")\nprint(f\" - {MODEL_DIR}/fd003_test_encoder_tsne_Diff.png\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}