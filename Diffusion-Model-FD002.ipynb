{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1579356,"sourceType":"datasetVersion","datasetId":933960}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TRAIN è³‡æ–™å‰è™•ç†","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport shap\n\n# === 1. å…¨å±€ SEEDï¼ˆç¢ºä¿ deterministicï¼‰===\nGLOBAL_SEED = 42\nrandom.seed(GLOBAL_SEED)\nnp.random.seed(GLOBAL_SEED)\n\n# === 2. åŸºæœ¬åƒæ•¸èˆ‡è·¯å¾‘ ===\nMODEL_DIR = '/kaggle/working'\nos.makedirs(MODEL_DIR, exist_ok=True)\nmax_rul = 130      # æœ€å¤§RULæˆªæ–·\nwindow_size = 12   # é€€åŒ–èµ·é»åµæ¸¬ window\nthreshold = 0.2    # é€€åŒ–åµæ¸¬ threshold\n\n# FD002: 26å€‹æ„Ÿæ¸¬å™¨ + 5æ¬„ (unit, time, op1, op2, op3)\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 27)]\n\n# === 3. è³‡æ–™è¼‰å…¥èˆ‡åˆæ­¥RULæ¨™è¨» ===\ndef load_and_label(filepath, max_rul=130):\n    df = pd.read_csv(filepath, sep='\\s+', header=None, dtype=float, engine='python')\n    print(f\"åŸå§‹ shape: {df.shape}\")\n    if df.shape[1] < len(columns):\n        for i in range(len(columns) - df.shape[1]):\n            df[f'extra_{i}'] = 0.0\n    if df.shape[1] > len(columns):\n        df = df.iloc[:, :len(columns)]\n    df.columns = columns\n    df['RUL_linear'] = df.groupby('unit')['time'].transform('max') - df['time']\n    df['RUL_linear'] = df['RUL_linear'].clip(upper=max_rul)\n    return df\n\n# ======= æŒ‡å®š FD002 è¨“ç·´é›†è·¯å¾‘ï¼======\ntrain = load_and_label('/kaggle/input/cmapssdata/train_FD002.txt', max_rul=max_rul)\nprint(\"Train loaded. Shape:\", train.shape)\n\n# === 4. åˆ†æ®µç·šæ€§é€€åŒ–RUL (Algorithm1) ===\ndef generate_piecewise_rul_algorithm1(df, feature_cols, window_size=12, threshold=0.2, max_rul=130, verbose=False):\n    result_dfs = []\n    for unit_id, group in df.groupby('unit'):\n        group = group.sort_values('time').reset_index(drop=True)\n        sensor_data = group[feature_cols].values\n        num_cycles = len(group)\n        num_windows = num_cycles // window_size\n\n        if num_windows < 3:\n            irul = num_cycles\n            group['iRUL'] = irul\n            group['RUL_piecewise'] = np.clip(irul - group['time'], 0, max_rul)\n            result_dfs.append(group)\n            continue\n\n        centroids = [np.mean(sensor_data[i * window_size:(i + 1) * window_size], axis=0)\n                     for i in range(num_windows)]\n        base = centroids[0]\n        degradation_found = False\n\n        for i in range(2, num_windows):\n            dist_sq = np.sum((centroids[i] - base) ** 2)\n            if verbose:\n                print(f\"[Unit {unit_id}] Compare w1 to w{i+1}: Dist^2 = {dist_sq:.4f}\")\n            if dist_sq >= threshold:\n                degradation_start = i * window_size\n                irul = num_cycles - degradation_start\n                degradation_found = True\n                break\n\n        if not degradation_found:\n            irul = num_cycles\n\n        rul_piecewise = [\n            irul if t <= (num_cycles - irul) else irul - (t - (num_cycles - irul))\n            for t in range(num_cycles)\n        ]\n        group['iRUL'] = irul\n        group['RUL_piecewise'] = np.clip(rul_piecewise, 0, max_rul)\n        result_dfs.append(group)\n\n    return pd.concat(result_dfs, ignore_index=True)\n\n# === 5. åŸºç¤å‰è™•ç†ï¼ˆç§»é™¤ä½è®Šç•°æ„Ÿæ¸¬å™¨ï¼‰===\nsensor_cols = [f's{i}' for i in range(1, 27)]\nsensor_cols_keep = [col for col in sensor_cols if train[col].std() >= 1e-3]\n\n# === 6. Z-score éæ¿¾é›¢ç¾¤å€¼ ===\ndef z_score_filter(df, cols, threshold=4.0):\n    for col in cols:\n        mean = df[col].mean()\n        std = df[col].std()\n        z = (df[col] - mean) / std\n        df.loc[z.abs() > threshold, col] = mean\n    return df\n\ntrain = z_score_filter(train, sensor_cols_keep)\n\n# === 7. åˆ†æ®µç·šæ€§RULæ¨™è¨» ===\ntrain = generate_piecewise_rul_algorithm1(\n    train, sensor_cols_keep, window_size=window_size, threshold=threshold, max_rul=max_rul, verbose=False\n)\n\n# === 8. ç‰¹å¾µé¸æ“‡ï¼ˆSHAP + çš®çˆ¾æ£®ï¼Œé˜²OOMç‰ˆï¼‰===\nprint(\"é–‹å§‹è¨“ç·´ RandomForestï¼ˆåƒ…ç”¨éƒ¨åˆ†è³‡æ–™ç‰¹å¾µåšSHAPï¼Œé˜²æ­¢OOMï¼‰\")\n# åªå–éƒ¨åˆ†è³‡æ–™åš SHAP ç‰¹å¾µåˆ†æ\nsample_n = min(2000, len(train))\ntrain_sample = train.sample(n=sample_n, random_state=GLOBAL_SEED)\nrf = RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1)\nrf.fit(train_sample[sensor_cols_keep], train_sample['RUL_piecewise'])\n\nshap_sample_n = min(200, len(train_sample))\nshap_sample_idx = np.random.choice(len(train_sample), shap_sample_n, replace=False)\nshap_X = train_sample[sensor_cols_keep].iloc[shap_sample_idx]\nprint(\"è¨ˆç®— SHAP ä¸­...\")\nexplainer = shap.Explainer(rf, shap_X)\nshap_values = explainer(shap_X, check_additivity=False)  # <== é—œé–‰åŠ ç¸½é©—è­‰\nmean_shap = np.abs(shap_values.values).mean(axis=0)\nshap_scores = pd.Series(mean_shap, index=sensor_cols_keep).sort_values(ascending=False)\ntop8_shap = shap_scores.head(8).index.tolist()\n\ncorrs = train[sensor_cols_keep + ['RUL_piecewise']].corr()['RUL_piecewise'].abs().sort_values(ascending=False)\ntop8_pearson = corrs.drop('RUL_piecewise').head(8).index.tolist()\n\nselected_sensors = [x for x in top8_shap if x in top8_pearson]\nfor x in top8_pearson:\n    if x not in selected_sensors:\n        selected_sensors.append(x)\n    if len(selected_sensors) == 8:\n        break\n\nprint(\"æœ€çµ‚é¸ç”¨çš„8å€‹æ„Ÿæ¸¬å™¨:\", selected_sensors)\n\n# === 9. æ§‹å»ºæœ€çµ‚ç‰¹å¾µï¼ˆå«OP/å·®åˆ†ï¼‰===\nfeatures = selected_sensors + ['op1', 'op2', 'op3']\n\ndef multi_exponential_smoothing(series, alphas=[0.1, 0.3]):\n    results = []\n    for alpha in alphas:\n        smoothed = [series.iloc[0]]\n        for n in range(1, len(series)):\n            smoothed.append(alpha * series.iloc[n] + (1 - alpha) * smoothed[-1])\n        results.append(pd.Series(smoothed, index=series.index))\n    return sum(results) / len(results)\n\nfor col in features:\n    train[col] = train.groupby('unit')[col].transform(lambda x: multi_exponential_smoothing(x)).astype('float64')\n\nfor col in selected_sensors:\n    train[f'{col}_diff'] = train.groupby('unit')[col].diff().fillna(0)\n\nfinal_features = features + [f'{col}_diff' for col in selected_sensors]\n\n# === 10. æ¨™æº–åŒ– ===\nscaler = StandardScaler()\ntrain[final_features] = scaler.fit_transform(train[final_features])\n\n# === 11. å„²å­˜æ‰€æœ‰é‡è¦è³‡è¨Š ===\njoblib.dump(scaler, f'{MODEL_DIR}/scaler_preprocessed.joblib')\njoblib.dump(final_features, f'{MODEL_DIR}/feature_names.pkl')\ntrain.to_csv(f'{MODEL_DIR}/train_with_piecewise_rul.csv', index=False)\n\nprint('ğŸš© å·²å®Œæˆåˆ†æ®µç·šæ€§RULè³‡æ–™èˆ‡æ‰€æœ‰ç‰¹å¾µè™•ç†ï¼Œæª”æ¡ˆå„²å­˜ï¼š')\nprint(f'- {MODEL_DIR}/train_with_piecewise_rul.csv')\nprint(f'- {MODEL_DIR}/feature_names.pkl')\nprint(f'- {MODEL_DIR}/scaler_preprocessed.joblib')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport joblib\nimport pandas as pd\n\nSEQ_LEN = 32\nMODEL_DIR = '/kaggle/working'\n\nfinal_features = joblib.load(f'{MODEL_DIR}/feature_names.pkl')\ntrain = pd.read_csv(f'{MODEL_DIR}/train_with_piecewise_rul.csv')\n\n# è‹¥çœŸçš„å¾ˆå¤§ï¼Œå¯ä»¥åˆ†æ‰¹è™•ç†å†np.save/appendï¼ˆå¦‚ä¸‹ç°¡åŒ–å–®æ‰¹è™•ç†ç‰ˆæœ¬ï¼‰\nX_list, y_list, unit_list = [], [], []\nunit_ids = train['unit'].unique()\n\n# å¦‚æ©Ÿå™¨è³‡æºæœ‰é™ï¼Œå¯åƒ…æ¸¬è©¦éƒ¨åˆ†unit\n# unit_ids = unit_ids[:20]   # å…ˆé©—è­‰å°æ‰¹é‡æµç¨‹å†å…¨é‡\n\nfor unit in unit_ids:\n    df_unit = train[train['unit'] == unit]\n    arr = df_unit[final_features].values\n    rul_piecewise = df_unit['RUL_piecewise'].values\n    if len(arr) < SEQ_LEN:\n        continue\n    for i in range(len(arr) - SEQ_LEN + 1):\n        X_list.append(arr[i:i + SEQ_LEN])\n        y_list.append(rul_piecewise[i + SEQ_LEN - 1])\n        unit_list.append(unit)\n\nX_all = np.stack(X_list).astype(np.float32)\ny_all = np.array(y_list).astype(np.float32)\nunit_all = np.array(unit_list).astype(int)\n\nprint(\"æ»‘å‹•è¦–çª—å®Œæˆ:\")\nprint(\"X_all shape:\", X_all.shape)\nprint(\"y_all shape:\", y_all.shape)\nprint(\"unit_all shape:\", unit_all.shape)\n\nnp.save(f'{MODEL_DIR}/X_all.npy', X_all)\nnp.save(f'{MODEL_DIR}/y_all.npy', y_all)\nnp.save(f'{MODEL_DIR}/unit_all.npy', unit_all)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage1 Encoder é è¨“ç·´","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, Input\nimport numpy as np\nimport os\n\ndef cbam_block(inputs, reduction_ratio=8):\n    channel = int(inputs.shape[-1])\n    avg_pool = layers.GlobalAveragePooling1D(keepdims=True)(inputs)\n    max_pool = layers.GlobalMaxPooling1D(keepdims=True)(inputs)\n    dense = layers.Dense(channel // reduction_ratio, activation='relu')\n    dense_out = layers.Dense(channel)\n    avg_out = dense_out(dense(avg_pool))\n    max_out = dense_out(dense(max_pool))\n    channel_attention = layers.Activation('sigmoid')(layers.Add()([avg_out, max_out]))\n    channel_refined = layers.Multiply()([inputs, channel_attention])\n    avg_pool_spatial = layers.GlobalAveragePooling1D(keepdims=True)(channel_refined)\n    max_pool_spatial = layers.GlobalMaxPooling1D(keepdims=True)(channel_refined)\n    concat = layers.Concatenate(axis=-1)([avg_pool_spatial, max_pool_spatial])\n    spatial_attention = layers.Conv1D(1, 7, padding='same', activation='sigmoid')(concat)\n    refined = layers.Multiply()([channel_refined, spatial_attention])\n    return refined\n\ndef resblock(x, filters, kernel_size=3, stride=1):\n    shortcut = x\n    x = layers.Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = cbam_block(x)\n    if int(shortcut.shape[-1]) != filters:\n        shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n    x = layers.Add()([shortcut, x])\n    x = layers.Activation('relu')(x)\n    return x\n\ndef align_and_concat(x1, x2):\n    t1 = x1.shape[1]\n    t2 = x2.shape[1]\n    minlen = min(t1, t2)\n    if t1 > minlen:\n        x1 = layers.Cropping1D((0, t1 - minlen))(x1)\n    if t2 > minlen:\n        x2 = layers.Cropping1D((0, t2 - minlen))(x2)\n    return layers.Concatenate()([x1, x2])\n\ndef build_unet_encoder(input_dim, embed_dim, seq_len, base_filters=32, depth=3):\n    input_x = Input(shape=(seq_len, input_dim), name='input_1')\n    input_tau = Input(shape=(embed_dim,), name='input_2')\n    t_proj = layers.Dense(input_dim)(input_tau)\n    t_proj_exp = layers.Reshape((1, input_dim))(t_proj)\n    x = layers.Add()([input_x, t_proj_exp])\n    skips = []\n    for d in range(depth):\n        filters = base_filters * (2 ** d)\n        x = resblock(x, filters)\n        skips.append(x)\n        if d != depth - 1:\n            x = layers.MaxPooling1D(2)(x)\n    x = resblock(x, filters * 2)\n    for d in reversed(range(depth)):\n        x = layers.UpSampling1D(2)(x)\n        x = align_and_concat(x, skips[d])\n        x = resblock(x, base_filters * (2 ** d))\n    output = layers.Conv1D(input_dim, 1, padding='same')(x)\n    model = Model([input_x, input_tau], output)\n    return model\n\nSEQ_LEN = 32\nD = 16\nT = 1000\nMODEL_DIR = '/kaggle/working'\nX_all = np.load(f'{MODEL_DIR}/X_all.npy')\nN_SENSORS = X_all.shape[-1]\nBATCH_SIZE = 128\nEPOCHS_STAGE1 = 300\n\ndef cosine_beta_schedule(timesteps, s=0.008):\n    steps = timesteps + 1\n    x = np.linspace(0, timesteps, steps)\n    f = np.cos(((x / timesteps) + s) / (1 + s) * np.pi / 2) ** 2\n    alphas_cumprod = f / f[0]\n    betas = np.clip(1 - (alphas_cumprod[1:] / alphas_cumprod[:-1]), 0, 0.999)\n    return betas\n\nbetas = cosine_beta_schedule(T, s=0.008)\nalphas = 1 - betas\nalphas_cumprod = np.cumprod(alphas)\n\ndef get_timestep_embedding(timesteps, dim=D):\n    timesteps = tf.convert_to_tensor(timesteps, dtype=tf.float32)\n    timesteps = tf.reshape(timesteps, [-1, 1])\n    half_dim = dim // 2\n    emb = tf.math.log(10000.0) / (half_dim - 1)\n    emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n    emb = timesteps * emb\n    emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n    return emb\n\ndef diffusion_dataset(X, batch_size, T, alphas_cumprod, D):\n    def generator():\n        dataset_size = len(X)\n        while True:\n            idxs = np.random.permutation(dataset_size)\n            for i in range(0, dataset_size, batch_size):\n                batch_idx = idxs[i:i+batch_size]\n                x_start = X[batch_idx]\n                b = len(x_start)\n                t = np.random.randint(1, T + 1, size=b)\n                tau = get_timestep_embedding(t, D).numpy()\n                noise = np.random.randn(*x_start.shape).astype(np.float32)\n                sqrt_alpha = np.sqrt(alphas_cumprod[t - 1])[:, None, None]\n                sqrt_one_minus_alpha = np.sqrt(1 - alphas_cumprod[t - 1])[:, None, None]\n                x_t = sqrt_alpha * x_start + sqrt_one_minus_alpha * noise\n                yield {\"input_1\": x_t, \"input_2\": tau}, noise\n    output_signature = (\n        {\"input_1\": tf.TensorSpec(shape=(None, SEQ_LEN, N_SENSORS), dtype=tf.float32),\n         \"input_2\": tf.TensorSpec(shape=(None, D), dtype=tf.float32)},\n        tf.TensorSpec(shape=(None, SEQ_LEN, N_SENSORS), dtype=tf.float32)\n    )\n    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n\ntrain_ds = diffusion_dataset(X_all, BATCH_SIZE, T, alphas_cumprod, D)\nsteps_per_epoch = len(X_all) // BATCH_SIZE\n\nunet_encoder = build_unet_encoder(N_SENSORS, D, SEQ_LEN)\nunet_encoder.compile(optimizer=tf.keras.optimizers.Adam(5e-4), loss='mse')\nunet_encoder.fit(\n    train_ds,\n    epochs=EPOCHS_STAGE1,\n    steps_per_epoch=steps_per_epoch,\n    callbacks=[tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)]\n)\n\ngap = layers.GlobalAveragePooling1D()(unet_encoder.layers[-2].output)\nencoder_embedding_model = Model(unet_encoder.inputs, gap)\nunet_encoder.save(f'{MODEL_DIR}/stage1_encoder_full.keras')\nencoder_embedding_model.save(f'{MODEL_DIR}/stage1_encoder_embedding.keras')\nprint(\"âœ… Stage1 Encoder/Embeddingå·²å„²å­˜\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage2 Optuna æœå°‹ + RUL MLP","metadata":{}},{"cell_type":"code","source":"!pip install -U optuna optuna-integration[tfkeras]\n\nimport tensorflow as tf\nimport optuna\nfrom optuna_integration.tfkeras import TFKerasPruningCallback\nimport keras\nimport numpy as np\nfrom tensorflow.keras import layers, Input, Model\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.metrics import mean_squared_error\nimport os\n\nMODEL_DIR = '/kaggle/working'\nSEQ_LEN, D, T = 32, 16, 1000\n\n# --- è¼‰å…¥è³‡æ–™ ---\nX_all = np.load(f'{MODEL_DIR}/X_all.npy')\ny_all = np.load(f'{MODEL_DIR}/y_all.npy')\nunit_all = np.load(f'{MODEL_DIR}/unit_all.npy')\n\n# === FD002 å®˜æ–¹åˆ†å‰²ï¼ˆä¸»æµè«–æ–‡/Leaderboardç”¨æ³•ï¼‰===\ntrain_units = set(range(1, 211))   # 1~210 è¨“ç·´\nval_units = set(range(211, 261))   # 211~260 é©—è­‰\n\nX_train, y_train, X_val, y_val = [], [], [], []\nfor x, y, u in zip(X_all, y_all, unit_all):\n    if u in train_units:\n        X_train.append(x)\n        y_train.append(y)\n    elif u in val_units:\n        X_val.append(x)\n        y_val.append(y)\nX_train, y_train = np.array(X_train), np.array(y_train)\nX_val, y_val = np.array(X_val), np.array(y_val)\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_val shape:\", X_val.shape)\n\n# --- timestep embedding ---\ndef get_timestep_embedding_np(timesteps, dim):\n    timesteps = np.array(timesteps).reshape(-1, 1)\n    half_dim = dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim) * -emb)\n    emb = timesteps * emb\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    return emb.astype(np.float32)\n\nt_train = np.random.randint(1, T + 1, size=len(X_train))\ntau_train = get_timestep_embedding_np(t_train, D)\nt_val = np.full(len(X_val), SEQ_LEN) # é©—è­‰æ™‚å¯å›ºå®šç”¨ SEQ_LEN\ntau_val = get_timestep_embedding_np(t_val, D)\n\n# --- è¼‰å…¥Encoder ---\nkeras.config.enable_unsafe_deserialization()\nencoder = tf.keras.models.load_model(f'{MODEL_DIR}/stage1_encoder_embedding.keras', compile=False)\nencoder.trainable = True\n\n# --- RUL MLP é ­ ---\ndef build_rul_predictor(input_dim, h1=256, h2=128, h3=64, dropout=0.2):\n    x_in = Input(shape=(input_dim,))\n    x = layers.Dense(h1, activation='relu')(x_in)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Dense(h2, activation='relu')(x)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Dense(h3, activation='relu')(x)\n    x = layers.Dropout(dropout)(x)\n    y_out = layers.Dense(1, activation='linear')(x)\n    return Model(x_in, y_out)\n\n# --- Optuna objective ---\ndef objective(trial):\n    lr = trial.suggest_float(\"learning_rate\", 1e-4, 3e-4, log=True)\n    h1 = trial.suggest_int(\"h1\", 256, 384, step=64)\n    h2 = trial.suggest_int(\"h2\", 96, 192, step=32)\n    h3 = trial.suggest_int(\"h3\", 32, 64, step=16)\n    dropout = trial.suggest_float(\"dropout\", 0.14, 0.22)\n    batch_size = trial.suggest_categorical(\"batch_size\", [32, 128])\n    patience_es = trial.suggest_int(\"patience_es\", 18, 25)\n    patience_rlr = trial.suggest_int(\"patience_rlr\", 5, 8)\n    lambda_high = trial.suggest_float(\"lambda_high\", 0.02, 0.08)\n    def custom_rul_loss(lambda_high=lambda_high):\n        def loss(y_true, y_pred):\n            base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n            high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n            return base + lambda_high * high_bias\n        return loss\n\n    input_x = Input(shape=(SEQ_LEN, X_train.shape[-1]))\n    input_t = Input(shape=(D,))\n    encoded = encoder([input_x, input_t])\n    mlp = build_rul_predictor(encoded.shape[-1], h1, h2, h3, dropout)\n    output = mlp(encoded)\n    joint_model = Model([input_x, input_t], output)\n    joint_model.compile(optimizer=AdamW(learning_rate=lr), loss=custom_rul_loss())\n    joint_model.fit([X_train, tau_train], y_train,\n                    validation_data=([X_val, tau_val], y_val),\n                    epochs=100, batch_size=batch_size, verbose=0,\n                    callbacks=[\n                        tf.keras.callbacks.EarlyStopping(patience=patience_es, restore_best_weights=True),\n                        tf.keras.callbacks.ReduceLROnPlateau(patience=patience_rlr, factor=0.5, min_lr=1e-7),\n                        TFKerasPruningCallback(trial, \"val_loss\")\n                    ])\n    y_pred = joint_model.predict([X_val, tau_val], batch_size=batch_size).flatten()\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    return rmse\n\n# --- Optuna æœå°‹ ---\nstudy = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective, n_trials=30, timeout=2080)\n\n# --- å–å¾—æœ€ä½³åƒæ•¸ retrain ---\nbest = study.best_params\ninput_x = Input(shape=(SEQ_LEN, X_train.shape[-1]))\ninput_t = Input(shape=(D,))\nencoded = encoder([input_x, input_t])\nmlp = build_rul_predictor(encoded.shape[-1], best[\"h1\"], best[\"h2\"], best[\"h3\"], best[\"dropout\"])\noutput = mlp(encoded)\n\ndef custom_rul_loss(lambda_high=best[\"lambda_high\"]):\n    def loss(y_true, y_pred):\n        base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n        high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n        return base + lambda_high * high_bias\n    return loss\n\nfinal_model = Model([input_x, input_t], output)\nfinal_model.compile(optimizer=AdamW(learning_rate=best['learning_rate']), loss=custom_rul_loss())\nfinal_model.fit([X_train, tau_train], y_train,\n                validation_data=([X_val, tau_val], y_val),\n                epochs=100, batch_size=best['batch_size'], verbose=2,\n                callbacks=[\n                    tf.keras.callbacks.EarlyStopping(patience=best['patience_es'], restore_best_weights=True),\n                    tf.keras.callbacks.ReduceLROnPlateau(patience=best['patience_rlr'], factor=0.5, min_lr=1e-7)\n                ])\nfinal_model.save(f'{MODEL_DIR}/stage2_joint_model_best.h5')\n\ny_pred = final_model.predict([X_val, tau_val], batch_size=best['batch_size']).flatten()\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\nprint(f\"âœ… æœ€ä½³åƒæ•¸: {best}\")\nprint(f\"âœ… é©—è­‰é›†æœ€å° RMSE (piecewise RUL): {rmse:.4f}\")\nprint(f\"ğŸ“ æ¨¡å‹å·²å„²å­˜è‡³: {MODEL_DIR}/stage2_joint_model_best.h5\")\n\nstudy.trials_dataframe().to_csv(f\"{MODEL_DIR}/optuna_search_results.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test è³‡æ–™é›†é è™•ç†","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\nimport os\n\nMODEL_DIR = '/kaggle/working'\nSEQ_LEN = 32\nprint(\"ä½¿ç”¨å›ºå®š SEQ_LEN:\", SEQ_LEN)\n\nscaler = joblib.load(f'{MODEL_DIR}/scaler_preprocessed.joblib')\nfeature_names = joblib.load(f'{MODEL_DIR}/feature_names.pkl')\n\n# 1. æ­£ç¢ºè¨­ç½® FD002 çš„æ¬„ä½å\ncolumns = ['unit', 'time', 'op1', 'op2', 'op3'] + [f's{i}' for i in range(1, 22)]\ntest = pd.read_csv('/kaggle/input/cmapssdata/test_FD002.txt', sep='\\s+', header=None)\nassert test.shape[1] == 26, f\"test_FD002.txt æ¬„æ•¸ {test.shape[1]} ä¸æ­£ç¢ºï¼ˆé æœŸ26ï¼‰\"\ntest.columns = columns\nrul_truth = pd.read_csv('/kaggle/input/cmapssdata/RUL_FD002.txt', header=None, names=['RUL'])\n\ndef exponential_smoothing(series, alpha=0.2):\n    result = [series.iloc[0]]\n    for n in range(1, len(series)):\n        result.append(alpha * series.iloc[n] + (1 - alpha) * result[-1])\n    return pd.Series(result, index=series.index)\n\n# 2. åªå°åŸç”Ÿ sensor èˆ‡ op åšå¹³æ»‘\nfor col in feature_names:\n    if col.endswith('_diff'):\n        continue\n    if col in test.columns:\n        test[col] = test.groupby('unit')[col].transform(lambda x: exponential_smoothing(x, alpha=0.2))\n\n# 3. è¨ˆç®—ä¸¦è£œé½Šå·®åˆ†æ¬„ä½\nfor col in feature_names:\n    if col.endswith('_diff'):\n        base_col = col.replace('_diff', '')\n        if base_col in test.columns:\n            test[col] = test.groupby('unit')[base_col].diff().fillna(0)\n        else:\n            test[col] = 0.0\n\n# 4. è‹¥æœ‰ç¼º feature_names è£¡çš„æ¬„ä½ï¼ˆé€™åœ¨optunaæµç¨‹ç­‰è‡ªå‹•ç”¢ç”Ÿç‰¹å¾µæ™‚å¸¸è¦‹ï¼‰ï¼Œè£œ0\nfor col in feature_names:\n    if col not in test.columns:\n        test[col] = 0.0\n\n# 5. æ¨™æº–åŒ–\ntest[feature_names] = scaler.transform(test[feature_names])\n\n# 6. è£½ä½œæ»‘å‹•è¦–çª—\nX_test_list, timestep_list, units_list = [], [], []\nfor unit in sorted(test['unit'].unique()):\n    df_unit = test[test['unit'] == unit]\n    arr = df_unit[feature_names].values\n    if len(arr) >= SEQ_LEN:\n        X_test_list.append(arr[-SEQ_LEN:])\n        timestep_list.append(df_unit['time'].values[-1])\n        units_list.append(unit)\nX_test = np.stack(X_test_list).astype(np.float32)\n\ndef get_timestep_embedding_np(timesteps, dim):\n    timesteps = np.array(timesteps).reshape(-1, 1)\n    half_dim = dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim) * -emb)\n    emb = timesteps * emb\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    return emb.astype(np.float32)\n\nD = 16\ntau_test = get_timestep_embedding_np(timestep_list, D)\ny_test = rul_truth['RUL'].values\n\nnp.save(f'{MODEL_DIR}/X_test.npy', X_test)\nnp.save(f'{MODEL_DIR}/tau_test.npy', tau_test)\nnp.save(f'{MODEL_DIR}/units_list.npy', np.array(units_list))\nnp.save(f'{MODEL_DIR}/y_test.npy', y_test)\nprint(f\"âœ… Test è¦–çª—æ•¸: {len(X_test)}\")\nprint(f\"ğŸ§ª å°é½Š unit æ•¸é‡: {len(np.unique(units_list))}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test è³‡æ–™é›†é©—è­‰","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport joblib\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.manifold import TSNE\nfrom tensorflow.keras.models import load_model\nimport tensorflow as tf\nimport os\n\nMODEL_DIR = '/kaggle/working'\nX_test = np.load(f'{MODEL_DIR}/X_test.npy')\ntau_test = np.load(f'{MODEL_DIR}/tau_test.npy')\nunits_list = np.load(f'{MODEL_DIR}/units_list.npy').astype(int)\nrul_truth = pd.read_csv('/kaggle/input/cmapssdata/RUL_FD002.txt', header=None, names=['RUL'])\ny_true = rul_truth.loc[units_list - 1, 'RUL'].values\n\noptuna_result_csv = f'{MODEL_DIR}/optuna_search_results.csv'\noptuna_df = pd.read_csv(optuna_result_csv)\nif 'params_lambda_high' in optuna_df.columns:\n    best_trial = optuna_df.loc[optuna_df['value'].idxmin()]\n    best_lambda_high = best_trial['params_lambda_high']\nelse:\n    best_lambda_high = 0.1\n\ndef custom_rul_loss(lambda_high=best_lambda_high):\n    def loss(y_true, y_pred):\n        base = tf.reduce_mean(tf.square(tf.clip_by_value(y_pred, 0, 130) - y_true))\n        high_bias = tf.reduce_mean(tf.square(tf.nn.relu(y_pred - 110)))\n        return base + lambda_high * high_bias\n    return loss\n\n# å¾©åŸæˆæ­£å¼ç‰ˆçš„ .h5 è·¯å¾‘\njoint_model = load_model(\n    f'{MODEL_DIR}/stage2_joint_model_best.h5',\n    custom_objects={'loss': custom_rul_loss()}\n)\n\ny_pred = joint_model.predict([X_test, tau_test], verbose=1).flatten()\ny_pred = np.clip(y_pred, 0, 130)\n\ndef nasa_score(y_true, y_pred):\n    score = 0\n    for true, pred in zip(y_true, y_pred):\n        d = pred - true\n        if d < 0:\n            score += np.exp(-d / 13) - 1\n        else:\n            score += np.exp(d / 10) - 1\n    return score\n\nrmse = np.sqrt(mean_squared_error(y_true, y_pred))\nscore = nasa_score(y_true, y_pred)\nprint(f'âœ… Test RMSE: {rmse:.4f}')\nprint(f'âœ… Test NASA Score: {score:.4f}')\n\ndf_out = pd.DataFrame({\n    'unit': units_list,\n    'True_RUL': y_true,\n    'Pred_RUL': y_pred,\n    'Error': y_pred - y_true\n})\ndf_out.to_csv(f'{MODEL_DIR}/test_pred_result.csv', index=False)\nprint(f\"ğŸ“ é æ¸¬çµæœå·²å„²å­˜ï¼š{MODEL_DIR}/test_pred_result.csv\")\n\nplt.figure(figsize=(8, 5))\nplt.plot(y_true, label='True RUL', marker='o', alpha=0.8)\nplt.plot(y_pred, label='Predicted RUL', marker='x', alpha=0.8)\nplt.xlabel('Test Unit (Engine)')\nplt.ylabel('RUL')\nplt.legend()\nplt.title(f'Predicted vs. True RUL (Test)\\nRMSE={rmse:.2f} | NASA Score={score:.2f}')\nplt.grid()\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/test_pred_vs_true_rul.png')\nplt.close()\n\nplt.figure(figsize=(7,5))\nplt.scatter(y_true, y_pred - y_true, alpha=0.75, c='royalblue', edgecolor='k')\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('True RUL')\nplt.ylabel('Prediction Error (Predicted - True)')\nplt.title('Residual Distribution (Error vs True RUL)')\nplt.grid()\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/test_residual_scatter.png')\nplt.close()\n\nplt.figure(figsize=(7,4))\nplt.hist(y_pred - y_true, bins=25, color='skyblue', edgecolor='black')\nplt.title('Prediction Error Histogram')\nplt.xlabel('Prediction Error (Predicted - True)')\nplt.ylabel('Count')\nplt.grid()\nplt.tight_layout()\nplt.savefig(f'{MODEL_DIR}/test_error_histogram.png')\nplt.close()\n\ntry:\n    encoder = load_model(f'{MODEL_DIR}/stage1_encoder_embedding.keras', compile=False)\n    features = encoder.predict([X_test, tau_test])\n    features_2d = TSNE(n_components=2, random_state=42).fit_transform(features)\n    plt.figure(figsize=(7,6))\n    sc = plt.scatter(features_2d[:,0], features_2d[:,1], c=y_true, cmap='viridis', s=20)\n    plt.colorbar(sc, label='True RUL')\n    plt.title(\"t-SNE of Encoder Embedding Features (Test set)\")\n    plt.xlabel(\"t-SNE Dim 1\")\n    plt.ylabel(\"t-SNE Dim 2\")\n    plt.savefig(f'{MODEL_DIR}/test_encoder_tsne.png')\n    plt.close()\nexcept Exception as e:\n    print(f\"[t-SNE å¯è¦–åŒ–å¤±æ•—] {e}\")\n\nprint(f\"ğŸ“ˆ åœ–ç‰‡å·²å„²å­˜ï¼š\\n{MODEL_DIR}/test_pred_vs_true_rul.png\\n{MODEL_DIR}/test_residual_scatter.png\\n{MODEL_DIR}/test_error_histogram.png\\n{MODEL_DIR}/test_encoder_tsne.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}